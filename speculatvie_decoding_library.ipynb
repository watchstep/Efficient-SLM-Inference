{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my local models\n",
    "MODELZOO = {\n",
    "    \"phi3-14b\" : \"./models/Phi-3-medium-4k-instruct\",\n",
    "    \"phi3-3.8b\" : \"./models/Phi-3-mini-4k-instruct\",\n",
    "    \"bloom-560m\": \"./models/bloom-560m\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-10 11:25:04 config.py:391] Async output processing is not supported with speculative decoding currently.\n",
      "INFO 09-10 11:25:04 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='./models/Phi-3-medium-4k-instruct', speculative_config=SpeculativeConfig(draft_model='./models/Phi-3-mini-4k-instruct', num_spec_tokens=7), tokenizer='./models/Phi-3-medium-4k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=100, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=./models/Phi-3-medium-4k-instruct, use_v2_block_manager=True, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=False)\n",
      "INFO 09-10 11:25:05 selector.py:240] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 09-10 11:25:05 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-10 11:25:06 selector.py:240] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 09-10 11:25:06 selector.py:116] Using XFormers backend.\n",
      "INFO 09-10 11:25:06 spec_decode_worker.py:162] Configuring SpecDecodeWorker with proposer=<class 'vllm.spec_decode.multi_step_worker.MultiStepWorker'>\n",
      "INFO 09-10 11:25:06 rejection_sampler.py:64] Use pytorch for rejection sampling.\n",
      "INFO 09-10 11:25:06 spec_decode_worker.py:176] Configuring SpecDecodeWorker with sampler=<class 'vllm.model_executor.layers.rejection_sampler.RejectionSampler'>\n",
      "INFO 09-10 11:25:06 model_runner.py:915] Starting to load model ./models/Phi-3-medium-4k-instruct...\n",
      "INFO 09-10 11:25:07 selector.py:240] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 09-10 11:25:07 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd2cb65cc1344e894ebf2e5d2a66786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-10 11:26:15 model_runner.py:926] Loading model weights took 26.0838 GB\n",
      "INFO 09-10 11:26:15 model_runner.py:915] Starting to load model ./models/Phi-3-mini-4k-instruct...\n",
      "INFO 09-10 11:26:15 selector.py:240] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 09-10 11:26:15 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641106732261481b8e54ce0257eda879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-10 11:26:32 model_runner.py:926] Loading model weights took 7.1183 GB\n",
      "INFO 09-10 11:26:35 gpu_executor.py:122] # GPU blocks: 182, # CPU blocks: 1310\n",
      "INFO 09-10 11:26:38 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-10 11:26:38 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-10 11:26:54 model_runner.py:1335] Graph capturing finished in 16 secs.\n",
      "INFO 09-10 11:26:59 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-10 11:26:59 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-10 11:27:11 model_runner.py:1335] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=MODELZOO[\"phi3-14b\"],\n",
    "    tensor_parallel_size=1,\n",
    "    speculative_model=MODELZOO[\"phi3-3.8b\"],\n",
    "    num_speculative_tokens=7,\n",
    "    use_v2_block_manager=True,\n",
    "    max_model_len=100,  # Decrease this value to match the cache limit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "    new_set = []\n",
    "    for i in range(len(data)):\n",
    "        temp = data[i]['message'][0]['content']\n",
    "        new_set.append(temp)\n",
    "    return new_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-10 11:31:28 scheduler.py:817] Input prompt (115 tokens) is too long and exceeds limit of 100\n",
      "WARNING 09-10 11:31:28 scheduler.py:817] Input prompt (101 tokens) is too long and exceeds limit of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 30/30 [00:04<00:00,  7.15it/s, est. speed input: 595.19 toks/s, output: 87.44 toks/s]\n"
     ]
    }
   ],
   "source": [
    "####### Section 3. Load data and Inference -> Performance evaluation part #######\n",
    "start = time.perf_counter()\n",
    "data = load_dataset(\"json\", data_files=\"./data/test_dataset.jsonl\")['train']\n",
    "data_list = data_preprocessing(data)\n",
    "outs = llm.generate(data_list, sampling_params)\n",
    "end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-10 11:31:32 scheduler.py:817] Input prompt (115 tokens) is too long and exceeds limit of 100\n",
      "WARNING 09-10 11:31:32 scheduler.py:817] Input prompt (101 tokens) is too long and exceeds limit of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 30/30 [00:02<00:00, 13.81it/s, est. speed input: 1150.22 toks/s, output: 168.99 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: 'Answer: Deep sea animals\\n\\nQuestion: \\n\\n'\n",
      "Generated text: 'answer: Gas can fill any container it is given, and liquid is the opposite'\n",
      "Generated text: '\\nanswer: they are genetically called to\\n\\n'\n",
      "Generated text: 'answer: south\\n\\nanswer: south'\n",
      "Generated text: '\\nanswer: An aircraft taking a trip\\n\\nexplanation: Kinetic'\n",
      "Generated text: 'answer: protozoa\\n\\n\\nquestion: What is the name of the'\n",
      "Generated text: 'answer: Green house\\n\\n\\nquestion: What is the name of the process'\n",
      "Generated text: ''\n",
      "Generated text: '\\nanswer: It holds 500 mL of water.\\n\\n'\n",
      "Generated text: 'answer: the air becomes arid\\n\\nexplanation=The air becomes'\n",
      "Generated text: 'answer: July\\n\\nexplanation: July'\n",
      "Generated text: '\\nanswer: speaking with a witness\\n\\n- response: speaking with a witness'\n",
      "Generated text: 'answer: shell\\n\\nexplanation=shell'\n",
      "Generated text: 'Answer: the final barrel is'\n",
      "Generated text: 'answer: particles of iron\\n\\n\\nquestion: The first step in the process'\n",
      "Generated text: 'answer: H2O haze\\n\\nexplanation: Dew is'\n",
      "Generated text: ''\n",
      "Generated text: 'answer: glucose\\n\\n\\nPlants need light for glucose'\n",
      "Generated text: '\\nanswer: help prevent the effects of erosion\\n\\nexplanation'\n",
      "Generated text: 'answer:'\n",
      "Generated text: 'answer: salvage pl'\n",
      "Generated text: 'answer: less energy used by the water heater\\n\\nex'\n",
      "Generated text: '\\nanswer: people driving cars might be una'\n",
      "Generated text: '\\nanswer: light from our closest star\\n\\n\\nQuestion: What is the'\n",
      "Generated text: 'answer: the darkness is greatest\\n\\n\\nquestion: What is the name of'\n",
      "Generated text: 'answer: Water\\n\\nanswer: None of the given choices boil at the'\n",
      "Generated text: 'answer: clothing\\n\\nquestion: What is the name of the process of'\n",
      "Generated text: '\\nanswer: a cut peony\\n\\n- response: A cut peony'\n",
      "Generated text: \"answer: an alligator's habitat\\n\\nexplan\"\n",
      "Generated text: '\\nanswer: has seeds outside'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0, top_p=0.95)\n",
    "\n",
    "outputs = llm.generate(data_list, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Answers =====\n",
      "Correct Answer: Deep sea animals\n",
      "Generated Answer: Answer: Deep sea animalsQuestion: \n",
      "Correct Answer: uses what it needs\n",
      "Generated Answer: Gas can fill any container it is given, and liquid is the opposite\n",
      "Correct Answer: they are genetically called to\n",
      "Generated Answer: they are genetically called to\n",
      "Correct Answer: south\n",
      "Generated Answer: southsouth\n",
      "Correct Answer: An aircraft taking a trip\n",
      "Generated Answer: An aircraft taking a tripexplanation: Kinetic\n",
      "Correct Answer: protozoa\n",
      "Generated Answer: protozoaquestion: What is the name of the\n",
      "Correct Answer: Green house\n",
      "Generated Answer: Green housequestion: What is the name of the process\n",
      "Correct Answer: it unfreezes, because it is cold-blooded\n",
      "Generated Answer: \n",
      " correct!!\n",
      "Correct Answer: It holds 500 mL of water\n",
      "Generated Answer: It holds 500 mL of water.\n",
      "Correct Answer: fluid spreads from pores\n",
      "Generated Answer: the air becomes aridexplanation=The air becomes\n",
      "Correct Answer: July\n",
      "Generated Answer: Julyexplanation: July\n",
      "Correct Answer: speaking with a witness\n",
      "Generated Answer: speaking with a witness- response: speaking with a witness\n",
      "Correct Answer: shell\n",
      "Generated Answer: shellexplanation=shell\n",
      "Correct Answer: the final barrel is gone, there supply is finished\n",
      "Generated Answer: Answer: the final barrel is\n",
      "Answer: the final barrel is correct!!\n",
      "Correct Answer: particles of iron\n",
      "Generated Answer: particles of ironquestion: The first step in the process\n",
      "Correct Answer: H2O haze\n",
      "Generated Answer: H2O hazeexplanation: Dew is\n",
      "Correct Answer: constellations to appear in one place in spring and another in fall\n",
      "Generated Answer: \n",
      " correct!!\n",
      "Correct Answer: glucose\n",
      "Generated Answer: glucosePlants need light for glucose\n",
      "Correct Answer: lead to less impacted soil\n",
      "Generated Answer: help prevent the effects of erosionexplanation\n",
      "Correct Answer: storms\n",
      "Generated Answer: answer:\n",
      "answer: correct!!\n",
      "Correct Answer: salvage plastic bottles instead of throwing them away\n",
      "Generated Answer: salvage pl\n",
      "Correct Answer: less energy used by the water heater\n",
      "Generated Answer: less energy used by the water heaterex\n",
      "Correct Answer: people driving cars might be unaware the animal is close by\n",
      "Generated Answer: people driving cars might be una\n",
      "Correct Answer: light from our closest star\n",
      "Generated Answer: light from our closest starQuestion: What is the\n",
      "Correct Answer: the darkness is greatest\n",
      "Generated Answer: the darkness is greatestquestion: What is the name of\n",
      "Correct Answer: Kool-Aid\n",
      "Generated Answer: WaterNone of the given choices boil at the\n",
      "Correct Answer: rosebuds\n",
      "Generated Answer: clothingquestion: What is the name of the process of\n",
      "Correct Answer: a cut peony\n",
      "Generated Answer: a cut peony- response: A cut peony\n",
      "Correct Answer: an alligator's habitat\n",
      "Generated Answer: an alligator's habitatexplan\n",
      "Correct Answer: has seeds outside the flesh, unlike the blueberry\n",
      "Generated Answer: has seeds outside\n",
      "===== Perf result =====\n",
      "Elapsed_time:  4.97843243740499\n",
      "Correctness: 4/30\n"
     ]
    }
   ],
   "source": [
    "####### Section 4. Accuracy (Just for leasderboard) #######\n",
    "print(\"===== Answers =====\")\n",
    "correct = 0\n",
    "for i, out in enumerate(outs):\n",
    "    correct_answer = data[i][\"answer\"]\n",
    "    # 생성된 답변에서 불필요한 텍스트 제거\n",
    "    answer = out.outputs[0].text\n",
    "    cleaned_answer = re.sub(r\"A:\\n\\n\\n### response ###\\n\\n|\\n### response ###\\n\\n|A: |\\nB:\", \"\", answer).lstrip().replace(\"\\n\",\"\")\n",
    "    cleaned_answer = cleaned_answer.replace(\"answer: \",\"\")\n",
    "    \n",
    "    # 정답과 출력된 답변을 비교\n",
    "    print(f\"Correct Answer: {correct_answer}\")\n",
    "    print(f\"Generated Answer: {cleaned_answer}\")\n",
    "    if answer == cleaned_answer:\n",
    "        correct += 1\n",
    "        print(answer,\"correct!!\")\n",
    " \n",
    "print(\"===== Perf result =====\")\n",
    "print(\"Elapsed_time: \", end-start)\n",
    "print(f\"Correctness: {correct}/{len(data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harikang",
   "language": "python",
   "name": "harikang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
