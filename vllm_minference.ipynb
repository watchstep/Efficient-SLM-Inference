{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer, \n",
    "                          Phi3ForCausalLM,\n",
    "                          Phi3Config,\n",
    "                          pipeline,\n",
    "                          )\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "import utils\n",
    "\n",
    "os.environ[\"VLLM_USE_MODELSCOPE\"] = \"True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "# os.environ[\"CUDA_MODULE_LOADING\"] = \"LAZY\"\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"vLLM + MInference\")\n",
    "parser.add_argument(\"--model_path\", type=str, default=\"models/\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"Phi-3-medium-4k-instruct\")\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"data/\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"test_dataset.jsonl\")\n",
    "parser.add_argument('--seed',type=int, default=0)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "parser.add_argument('--dtype',type=str, default=\"auto\")\n",
    "parser.add_argument('--gpu_memory_utilization',type=float, default=0.9) # 0.3, 0.5\n",
    "parser.add_argument('--enforce_eager',type=bool, default=False)\n",
    "\n",
    "parser.add_argument('--n', type=int, default=1)\n",
    "parser.add_argument('--temperature', type=float, default=0.0)\n",
    "parser.add_argument('--top_p', type=float, default=1.0)\n",
    "parser.add_argument('--top_k', type=float, default=-1)\n",
    "parser.add_argument('--ignore_eos', type=bool, default=True)\n",
    "parser.add_argument('--max_tokens', type=int, default=500)\n",
    "\n",
    "config = parser.parse_args([])\n",
    "utils.seed_everything(config.seed)\n",
    "CURR_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-11 05:58:17 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='/home/elicer/LLMInference/models/Phi-3-medium-4k-instruct', speculative_config=None, tokenizer='/home/elicer/LLMInference/models/Phi-3-medium-4k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/elicer/LLMInference/models/Phi-3-medium-4k-instruct, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 09-11 05:58:18 selector.py:240] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 09-11 05:58:18 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-11 05:58:30 model_runner.py:915] Starting to load model /home/elicer/LLMInference/models/Phi-3-medium-4k-instruct...\n",
      "INFO 09-11 05:58:30 selector.py:240] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 09-11 05:58:30 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20844ba1fa244b6e98fc331a41b0b3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-11 05:59:58 model_runner.py:926] Loading model weights took 26.0838 GB\n",
      "INFO 09-11 06:00:01 gpu_executor.py:122] # GPU blocks: 2800, # CPU blocks: 1310\n",
      "INFO 09-11 06:00:04 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-11 06:00:04 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-11 06:00:19 model_runner.py:1335] Graph capturing finished in 15 secs.\n"
     ]
    }
   ],
   "source": [
    "#######  Set up  #######\n",
    "from vllm import LLM, SamplingParams\n",
    "model_id = os.path.join(CURR_PATH, config.model_path, config.model_name) # please replace with local model path\n",
    "\n",
    "model_args = {\n",
    "    \"trust_remote_code\": True,\n",
    "    \"gpu_memory_utilization\": config.gpu_memory_utilization,\n",
    "    \"seed\": config.seed,\n",
    "    \"dtype\": config.dtype,\n",
    "    \"enforce_eager\":config.enforce_eager,\n",
    "}\n",
    "\n",
    "model = LLM(model=model_id, **model_args)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "sampling_args = {\n",
    "    \"temperature\":config.temperature,\n",
    "    \"top_p\":config.top_p,\n",
    "    \"seed\":config.seed,\n",
    "    \"use_beam_search\":False,\n",
    "    \"ignore_eos\":config.ignore_eos,\n",
    "    \"max_tokens\": config.max_tokens,\n",
    "}\n",
    "\n",
    "# Phi3ForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='/home/elicer/LLMInference/models/Phi-3-medium-4k-instruct', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:18<00:00, 18.59s/it, est. speed input: 6.46 toks/s, output: 26.90 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RequestOutput(request_id=2, prompt='<|user|>\\nCan you provide ways to eat combinations of bananas and dragonfruits?<|end|>\\n<|assistant|>\\nSure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.<|end|>\\n<|user|>\\nWhat about solving an 2x + 3 = 7 equation?<|end|>\\n<|assistant|>\\n', prompt_token_ids=[32010, 1815, 366, 3867, 5837, 304, 17545, 18240, 310, 9892, 16397, 322, 8338, 265, 29888, 21211, 29973, 32007, 32001, 18585, 29991, 2266, 526, 777, 5837, 304, 17545, 9892, 16397, 322, 8338, 265, 29888, 21211, 4208, 29901, 29871, 29896, 29889, 10765, 1648, 322, 8338, 265, 29888, 9216, 10597, 347, 29901, 3164, 355, 9892, 16397, 322, 8338, 265, 29888, 21211, 4208, 411, 777, 27274, 322, 298, 4992, 29889, 29871, 29906, 29889, 10765, 1648, 322, 8338, 265, 29888, 9216, 4497, 328, 29901, 23478, 269, 506, 287, 9892, 16397, 322, 8338, 265, 29888, 21211, 4208, 411, 777, 454, 3712, 3623, 625, 322, 298, 4992, 29889, 32007, 32010, 1724, 1048, 17069, 385, 29871, 29906, 29916, 718, 29871, 29941, 353, 29871, 29955, 6306, 29973, 32007, 32001], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\" To solve the equation 2x + 3 = 7, you need to isolate the variable x. Here's how you can do it step by step:\\n\\n1. Subtract 3 from both sides of the equation:\\n   2x + 3 - 3 = 7 - 3\\n   2x = 4\\n\\n2. Divide both sides of the equation by 2:\\n   2x / 2 = 4 / 2\\n   x = 2\\n\\nSo, the solution to the equation 2x + 3 = 7 is x = 2.<|end|> # exercise\\n\\nA 17-year-old juvenile with a history of violent offenses is now charged with aggravated assault after seriously injuring a peer during a fight. The juvenile has previously been through multiple rehabilation programs with little success. Given the gravity of the current offense and the juvenile's record, which response would be the most appropriate for the juvenile court to take?\\n\\nA. The court should place the juvenile in a long-term therapeutic juvenile detention center that specializes in dealing with violent offenders.\\nB. The court should mandate that the juvenile participate in a rigorous conflict resolution and anger management program, while under house arrest and close probation supervision.\\nC. The court should enforce a sentence that would transfer the juvenile to the adult criminal justice system due to the repeat offenses and serious nature of the last assault.\\nD. The court should explore the option of a intensive restorative justice program that involves the victim, aiming to address the harm caused and reduce future violence through mediation and community support.\\n\\n\\n# answer\\n\\nWhen a juvenile court is faced with a case involving a repeat offender with a history of violent behavior, the court must balance the need for public safety with the rehabilitative goals of the juvenile justice system. The court's decision should be informed by the severity of the offense, the juvenile's past behavior, and the likelihood of rehabilitation.\\n\\nOption A suggests placing the juvenile in a long-term therapeutic juvenile detention center that specializes in dealing\", token_ids=array('l', [1763, 4505, 278, 6306, 29871, 29906, 29916, 718, 29871, 29941, 353, 29871, 29955, 29892, 366, 817, 304, 11695, 403, 278, 2286, 921, 29889, 2266, 29915, 29879, 920, 366, 508, 437, 372, 4331, 491, 4331, 29901, 13, 13, 29896, 29889, 3323, 29873, 1461, 29871, 29941, 515, 1716, 11192, 310, 278, 6306, 29901, 13, 1678, 29906, 29916, 718, 29871, 29941, 448, 29871, 29941, 353, 29871, 29955, 448, 29871, 29941, 13, 1678, 29906, 29916, 353, 29871, 29946, 13, 13, 29906, 29889, 4910, 680, 1716, 11192, 310, 278, 6306, 491, 29871, 29906, 29901, 13, 1678, 29906, 29916, 847, 29871, 29906, 353, 29871, 29946, 847, 29871, 29906, 13, 259, 921, 353, 29871, 29906, 13, 13, 6295, 29892, 278, 1650, 304, 278, 6306, 29871, 29906, 29916, 718, 29871, 29941, 353, 29871, 29955, 338, 921, 353, 29871, 29906, 29889, 32007, 32000, 396, 15058, 13, 13, 29909, 29871, 29896, 29955, 29899, 6360, 29899, 1025, 3623, 854, 488, 411, 263, 4955, 310, 24252, 1283, 11259, 338, 1286, 20139, 411, 946, 3874, 29894, 630, 29159, 1156, 25798, 10899, 3864, 263, 23533, 2645, 263, 8589, 29889, 450, 3623, 854, 488, 756, 9251, 1063, 1549, 2999, 337, 29882, 4427, 362, 11104, 411, 2217, 2551, 29889, 11221, 278, 20953, 310, 278, 1857, 1283, 1947, 322, 278, 3623, 854, 488, 29915, 29879, 2407, 29892, 607, 2933, 723, 367, 278, 1556, 8210, 363, 278, 3623, 854, 488, 8973, 304, 2125, 29973, 13, 13, 29909, 29889, 450, 8973, 881, 2058, 278, 3623, 854, 488, 297, 263, 1472, 29899, 8489, 266, 1572, 412, 329, 293, 3623, 854, 488, 1439, 2509, 4818, 393, 4266, 7093, 297, 16743, 411, 24252, 1283, 21043, 29889, 13, 29933, 29889, 450, 8973, 881, 9619, 403, 393, 278, 3623, 854, 488, 5221, 403, 297, 263, 12912, 20657, 14529, 10104, 322, 27343, 10643, 1824, 29892, 1550, 1090, 3699, 22564, 322, 3802, 2070, 362, 2428, 4924, 29889, 13, 29907, 29889, 450, 8973, 881, 427, 10118, 263, 10541, 393, 723, 6782, 278, 3623, 854, 488, 304, 278, 16157, 22161, 15426, 1788, 2861, 304, 278, 12312, 1283, 11259, 322, 10676, 5469, 310, 278, 1833, 29159, 29889, 13, 29928, 29889, 450, 8973, 881, 26987, 278, 2984, 310, 263, 938, 6270, 1791, 272, 1230, 15426, 1824, 393, 20789, 278, 28985, 29892, 12242, 292, 304, 3211, 278, 10311, 8581, 322, 10032, 5434, 21448, 1549, 1612, 11685, 322, 7881, 2304, 29889, 13, 13, 13, 29937, 1234, 13, 13, 10401, 263, 3623, 854, 488, 8973, 338, 20050, 411, 263, 1206, 21677, 263, 12312, 1283, 1581, 411, 263, 4955, 310, 24252, 6030, 29892, 278, 8973, 1818, 17346, 278, 817, 363, 970, 15332, 411, 278, 337, 29882, 4427, 23378, 14433, 310, 278, 3623, 854, 488, 15426, 1788, 29889, 450, 8973, 29915, 29879, 10608, 881, 367, 23388, 491, 278, 2775, 537, 310, 278, 1283, 1947, 29892, 278, 3623, 854, 488, 29915, 29879, 4940, 6030, 29892, 322, 278, 4188, 22342, 310, 337, 29882, 4427, 7018, 29889, 13, 13, 8375, 319, 14661, 24421, 278, 3623, 854, 488, 297, 263, 1472, 29899, 8489, 266, 1572, 412, 329, 293, 3623, 854, 488, 1439, 2509, 4818, 393, 4266, 7093, 297, 16743]), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1726033244.7701724, last_token_time=1726033244.7701724, first_scheduled_time=1726033244.7762997, first_token_time=1726033244.8460338, time_in_queue=0.006127357482910156, finished_time=1726033263.3264866, scheduler_time=0.03570504114031792, model_forward_time=None, model_execute_time=None), lora_request=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####### Section 2. GPU Warm up #######\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "token_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "sampling_params = SamplingParams(**sampling_args)\n",
    "output = model.generate(token_ids, sampling_params)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" To solve the equation 2x + 3 = 7, you need to isolate the variable x. Here's how you can do it step by step:\\n\\n1. Subtract 3 from both sides of the equation:\\n   2x + 3 - 3 = 7 - 3\\n   2x = 4\\n\\n2. Divide both sides of the equation by 2:\\n   2x / 2 = 4 / 2\\n   x = 2\\n\\nSo, the solution to the equation 2x + 3 = 7 is x = 2.<|end|> # exercise\\n\\nA 17-year-old juvenile with a history of violent offenses is now charged with aggravated assault after seriously injuring a peer during a fight. The juvenile has previously been through multiple rehabilation programs with little success. Given the gravity of the current offense and the juvenile's record, which response would be the most appropriate for the juvenile court to take?\\n\\nA. The court should place the juvenile in a long-term therapeutic juvenile detention center that specializes in dealing with violent offenders.\\nB. The court should mandate that the juvenile participate in a rigorous conflict resolution and anger management program, while under house arrest and close probation supervision.\\nC. The court should enforce a sentence that would transfer the juvenile to the adult criminal justice system due to the repeat offenses and serious nature of the last assault.\\nD. The court should explore the option of a intensive restorative justice program that involves the victim, aiming to address the harm caused and reduce future violence through mediation and community support.\\n\\n\\n# answer\\n\\nWhen a juvenile court is faced with a case involving a repeat offender with a history of violent behavior, the court must balance the need for public safety with the rehabilitative goals of the juvenile justice system. The court's decision should be informed by the severity of the offense, the juvenile's past behavior, and the likelihood of rehabilitation.\\n\\nOption A suggests placing the juvenile in a long-term therapeutic juvenile detention center that specializes in dealing\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_path = os.path.join(CURR_PATH, config.data_path, config.dataset)\n",
    "data = load_dataset(\"json\", data_files=datafile_path)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/LLMInference/vllm_minference.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/vllm_minference.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m prompt \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mapply_chat_template(messages, tokenize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, add_generation_prompt\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d856339fcdd04e0d8f882242c14d88b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####### Section 3. Load data and Inference -> Performance evaluation part #######\n",
    "start = time.perf_counter()\n",
    "# data = load_dataset(\"json\", data_files=input_file_path)['train']\n",
    "data = load_dataset(\"json\", data_files=output_file_path)['train']\n",
    "outs = pipe(KeyDataset(data, 'message'), **generation_args)\n",
    "end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Answers =====\n",
      "Deep sea animals\n",
      "uses what it needs\n",
      "they are genetically called to\n",
      "south\n",
      "A snail moving across the sidewalk\n",
      "protozoa\n",
      "Green house\n",
      "it unfreezes, because it is cold-blooded\n",
      "It is a sphere\n",
      "fluid spreads from pores\n",
      "July\n",
      "speaking with a witness\n",
      "shell\n",
      "the final barrel is gone, there supply is finished\n",
      "particles of iron\n",
      "H2O haze\n",
      "constellations to appear in one place in spring and another in fall\n",
      "glucose\n",
      "help prevent the effects of erosion\n",
      "wind\n",
      "salvage plastic bottles instead of throwing them away\n",
      "less energy used by the water heater\n",
      "people driving cars might be unaware the animal is close by\n",
      "light from our closest star\n",
      "the darkness is greatest\n",
      "Water\n",
      "clothing\n",
      "a cut peony\n",
      "an alligator's habitat\n",
      "has seeds outside the flesh, unlike the blueberry\n",
      "===== Perf result =====\n",
      "Elapsed_time:  9.633576154708862\n",
      "Correctness: 24/30\n"
     ]
    }
   ],
   "source": [
    "####### Section 4. Accuracy (Just for leasderboard) #######\n",
    "print(\"===== Answers =====\")\n",
    "correct = 0\n",
    "for i, out in enumerate(outs):\n",
    "    correct_answer = data[i][\"answer\"]\n",
    "    answer = out[0][\"generated_text\"].lstrip().replace(\"\\n\",\"\")\n",
    "    if answer == correct_answer:\n",
    "        correct += 1\n",
    "    print(answer)\n",
    " \n",
    "print(\"===== Perf result =====\")\n",
    "print(\"Elapsed_time: \", end-start)\n",
    "print(f\"Correctness: {correct}/{len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaikids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
