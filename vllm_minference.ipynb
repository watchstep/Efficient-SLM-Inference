{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer, \n",
    "                          Phi3ForCausalLM,\n",
    "                          Phi3Config,\n",
    "                          pipeline,\n",
    "                          )\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "import utils\n",
    "\n",
    "# os.environ[\"VLLM_USE_MODELSCOPE\"] = \"True\"\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "# os.environ[\"CUDA_MODULE_LOADING\"] = \"LAZY\"\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"vLLM + MInference\")\n",
    "parser.add_argument(\"--model_path\", type=str, default=\"models/\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"Phi-3-medium-4k-instruct\")\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"data/\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"test_dataset.jsonl\")\n",
    "parser.add_argument('--seed',type=int, default=0)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "parser.add_argument('--dtype',type=str, default=\"auto\")\n",
    "parser.add_argument('--gpu_memory_utilization',type=float, default=0.9) # 0.3, 0.5\n",
    "parser.add_argument('--enforce_eager',type=bool, default=False)\n",
    "\n",
    "parser.add_argument('--n', type=int, default=1)\n",
    "parser.add_argument('--temperature', type=float, default=0.0)\n",
    "parser.add_argument('--top_p', type=float, default=1.0)\n",
    "parser.add_argument('--top_k', type=float, default=-1)\n",
    "parser.add_argument('--ignore_eos', type=bool, default=True)\n",
    "parser.add_argument('--max_tokens', type=int, default=500)\n",
    "\n",
    "config = parser.parse_args([])\n",
    "utils.seed_everything(config.seed)\n",
    "CURR_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######  Set up  #######\n",
    "from vllm import LLM, SamplingParams\n",
    "model_id = os.path.join(CURR_PATH, config.model_path, config.model_name) # please replace with local model path\n",
    "\n",
    "model_args = {\n",
    "    \"trust_remote_code\": True,\n",
    "    \"gpu_memory_utilization\": config.gpu_memory_utilization,\n",
    "    \"seed\": config.seed,\n",
    "    \"dtype\": config.dtype,\n",
    "    \"enforce_eager\": config.enforce_eager,\n",
    "}\n",
    "\n",
    "model = LLM(model=model_id, **model_args)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "sampling_args = {\n",
    "    \"temperature\": config.temperature,\n",
    "    \"top_p\": config.top_p,\n",
    "    \"seed\": config.seed,\n",
    "    \"use_beam_search\":False,\n",
    "    \"ignore_eos\": config.ignore_eos,\n",
    "    \"max_tokens\": config.max_tokens,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Section 2. GPU Warm up #######\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "token_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "sampling_params = SamplingParams(**sampling_args)\n",
    "output = model.generate(token_ids, sampling_params)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_path = os.path.join(CURR_PATH, config.data_path, config.dataset)\n",
    "data = load_dataset(\"json\", data_files=datafile_path)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Section 3. Load data and Inference -> Performance evaluation part #######\n",
    "start = time.perf_counter()\n",
    "# data = load_dataset(\"json\", data_files=input_file_path)['train']\n",
    "data = load_dataset(\"json\", data_files=output_file_path)['train']\n",
    "outs = pipe(KeyDataset(data, 'message'), **generation_args)\n",
    "end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Section 4. Accuracy (Just for leasderboard) #######\n",
    "print(\"===== Answers =====\")\n",
    "correct = 0\n",
    "for i, out in enumerate(outs):\n",
    "    correct_answer = data[i][\"answer\"]\n",
    "    answer = out[0][\"generated_text\"].lstrip().replace(\"\\n\",\"\")\n",
    "    if answer == correct_answer:\n",
    "        correct += 1\n",
    "    print(answer)\n",
    " \n",
    "print(\"===== Perf result =====\")\n",
    "print(\"Elapsed_time: \", end-start)\n",
    "print(f\"Correctness: {correct}/{len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaikids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
