{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/modelopt/torch/quantization/tensor_quant.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  scaled_e4m3_abstract = torch.library.impl_abstract(\"trt::quantize_fp8\")(\n",
      "/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import torch_tensorrt\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "\n",
    "model_id = './cuda-fp16/'\n",
    "model = og.Model(model_id)\n",
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()\n",
    "\n",
    "search_options = {\"max_length\": 1046,\"temperature\":0.0}\n",
    "params = og.GeneratorParams(model)\n",
    "params.set_search_options(**search_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from llmlingua import PromptCompressor\n",
    "\n",
    "llm_lingua = PromptCompressor(\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\", \n",
    "                        device_map=\"auto\",\n",
    "                        use_llmlingua2=True)\n",
    "\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "data = load_dataset(\"json\", data_files=\"./data/test_dataset.jsonl\")['train']\n",
    "messages = data['message']\n",
    "\n",
    "# system_message = \"You are a helpful AI Assistant. Help users by replying to their queries and make sure the responses are polite. Do not hallucinate.\"\n",
    "# PROMPT = f\"<|system|>\\n{system_message}<|end|>\"                                        \n",
    "# token_ids.insert(0, PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def compress(content):\n",
    "    data = content.rpartition('choices:')\n",
    "\n",
    "    comp_dict = llm_lingua.compress_prompt(\n",
    "        context=data[0],\n",
    "        rate=0.7,\n",
    "    )\n",
    "\n",
    "    comp = comp_dict['compressed_prompt']\n",
    "    return f\"{comp}\\nchoices:{data[2]}\\n\"\n",
    "    \n",
    "def compressed_jsonl(input_file_path, output_file_path):\n",
    "    with open(input_file_path, 'r') as reader:\n",
    "        with open(output_file_path, 'w') as writer:\n",
    "            for line in reader:\n",
    "                line = json.loads(line)\n",
    "                line[\"message\"][0][\"content\"] = compress(line[\"message\"][0][\"content\"])\n",
    "                \n",
    "                json.dump(line, writer)\n",
    "                writer.write(\"\\n\")\n",
    "\n",
    "compressed_jsonl('./data/test_dataset.jsonl', './data/compressed.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "data = load_dataset(\"json\", data_files=\"./data/test_dataset.jsonl\")['train']\n",
    "messages = data['message']\n",
    "\n",
    "token_ids = hf_tokenizer.apply_chat_template(messages, \n",
    "                                        add_generation_prompt=True, \n",
    "                                        tokenize=False,)\n",
    "\n",
    "system_message = \"You are a helpful AI Assistant. Help users by replying to their queries and make sure the responses are polite. Do not hallucinate.\"\n",
    "PROMPT = f\"<|system|>\\n{system_message}<|end|>\"                                        \n",
    "token_ids.insert(0, PROMPT)\n",
    "\n",
    "outs = []\n",
    "for token_id in token_ids[1:]:\n",
    "    input_tokens = tokenizer.encode(token_id)\n",
    "    params = og.GeneratorParams(model)\n",
    "    params.input_ids = input_tokens\n",
    "    generator = og.Generator(model, params)\n",
    "\n",
    "    text = ''\n",
    "    while not generator.is_done():\n",
    "        generator.compute_logits()\n",
    "        generator.generate_next_token()\n",
    "        \n",
    "        new_token = generator.get_next_tokens()[0]\n",
    "        text += tokenizer_stream.decode(new_token)\n",
    "    \n",
    "    outs.append(\n",
    "        [{'generated_text': text}]\n",
    "        )\n",
    "\n",
    "end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Answers =====\n",
      "Correct Answer: Deep sea animals\n",
      "Generated Answer: Deep sea animals\n",
      "Correct Answer: uses what it needs\n",
      "Generated Answer: is standard weight andsize\n",
      "Correct Answer: they are genetically called to\n",
      "Generated Answer: they are genetically called to\n",
      "Correct Answer: south\n",
      "Generated Answer: south\n",
      "Correct Answer: An aircraft taking a trip\n",
      "Generated Answer: An aircraft taking a trip\n",
      "Correct Answer: protozoa\n",
      "Generated Answer: protozo\n",
      "Correct Answer: Green house\n",
      "Generated Answer: Green house\n",
      "Correct Answer: it unfreezes, because it is cold-blooded\n",
      "Generated Answer: it unfreezes, because it is cold-\n",
      "Correct Answer: It holds 500 mL of water\n",
      "Generated Answer: It holds mL of water\n",
      "Correct Answer: fluid spreads from pores\n",
      "Generated Answer: the air becomes arid\n",
      "Correct Answer: July\n",
      "Generated Answer: July\n",
      "Correct Answer: speaking with a witness\n",
      "Generated Answer: speaking with a\n",
      "Correct Answer: shell\n",
      "Generated Answer: shell\n",
      "Correct Answer: the final barrel is gone, there supply is finished\n",
      "Generated Answer: the final barrel is gone, there supply is finished\n",
      "Correct Answer: particles of iron\n",
      "Generated Answer: particles of\n",
      "Correct Answer: H2O haze\n",
      "Generated Answer: H2O haze\n",
      "Correct Answer: constellations to appear in one place in spring and another in fall\n",
      "Generated Answer: constellations to appear in one place in spring and another in fall\n",
      "Correct Answer: glucose\n",
      "Generated Answer: gluc\n",
      "Correct Answer: lead to less impacted soil\n",
      "Generated Answer: help prevent the effects of erosTo clarify, the correct answer is: help prevent the effects of erosion.\n",
      "Correct Answer: storms\n",
      "Generated Answer: wind\n",
      "Correct Answer: salvage plastic bottles instead of throwing them away\n",
      "Generated Answer: salvage pl bottles instead of throwing them away\n",
      "Correct Answer: less energy used by the water heater\n",
      "Generated Answer: less energy used by  water heater\n",
      "Correct Answer: people driving cars might be unaware the animal is close by\n",
      "Generated Answer: people driving cars might be unaware the animal is close by\n",
      "Correct Answer: light from our closest star\n",
      "Generated Answer: light from our closest star\n",
      "Correct Answer: the darkness is greatest\n",
      "Generated Answer: the darkness is greatest\n",
      "Correct Answer: Kool-Aid\n",
      "Generated Answer: Water\n",
      "Correct Answer: rosebuds\n",
      "Generated Answer: clothing\n",
      "Correct Answer: a cut peony\n",
      "Generated Answer: a cut peony\n",
      "Correct Answer: an alligator's habitat\n",
      "Generated Answer: an alligator's habitat\n",
      "Correct Answer: has seeds outside the flesh, unlike the blueberry\n",
      "Generated Answer: has seeds outside the flesh, unlike the berry\n",
      "Correct Answer: False\n",
      "Generated Answer: True\n",
      "===== Perf result =====\n",
      "Elapsed_time:  10.376926628989168\n",
      "Correctness: 15/31\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Answers =====\")\n",
    "correct = 0\n",
    "for i, out in enumerate(outs):\n",
    "    correct_answer = data[i][\"answer\"]\n",
    "    answer = out[0][\"generated_text\"].lstrip().replace(\"\\n\",\"\")\n",
    "\n",
    "    print(f\"Correct Answer: {correct_answer}\")\n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "    if answer == correct_answer:\n",
    "        correct += 1\n",
    "    \n",
    "print(\"===== Perf result =====\")\n",
    "print(\"Elapsed_time: \", end-start)\n",
    "print(f\"Correctness: {correct}/{len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaikids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
