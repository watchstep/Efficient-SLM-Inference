{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_tensorrt.dynamo.conversion.aten_ops_converters:Unable to import quantization op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxruntime import InferenceSession\n",
    "\n",
    "import warnings\n",
    "\n",
    "import argparse\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer, \n",
    "                          Phi3ForCausalLM,\n",
    "                          Phi3Config,\n",
    "                          pipeline,\n",
    "                          )\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch_tensorrt \n",
    "import utils\n",
    "import torch.backends.cudnn as cudnn\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else cpu )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 경고 무시\n",
    "warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"TensorRT\")\n",
    "parser.add_argument(\"--model_path\", type=str, default=\"models/\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"Phi-3-medium-4k-instruct\")\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"data/\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"test_dataset.jsonl\")\n",
    "parser.add_argument('--seed',type=int, default=0)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "parser.add_argument('--dtype',type=str, default=\"auto\")\n",
    "parser.add_argument('--n', type=int, default=1)\n",
    "parser.add_argument('--temperature', type=float, default=0.0)\n",
    "\n",
    "config = parser.parse_args([])\n",
    "\n",
    "utils.seed_everything(config.seed)\n",
    "CURR_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0722c339129f46a19392208325408e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":838, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/LLMInference/onnx_trt.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/onnx_trt.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_id, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/onnx_trt.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m max_memory \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/onnx_trt.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m0\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m28GB\u001b[39m\u001b[39m\"\u001b[39m,  \u001b[39m# GPU 0 \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/onnx_trt.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m28GB\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# CPU \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/onnx_trt.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m }\n\u001b[0;32m---> <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/onnx_trt.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m llm \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/onnx_trt.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     model_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/onnx_trt.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16,\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/onnx_trt.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/onnx_trt.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/onnx_trt.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     max_memory \u001b[39m=\u001b[39;49m max_memory \n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/onnx_trt.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/hrenv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mregister(config\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, model_class, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 559\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    560\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    561\u001b[0m     )\n\u001b[1;32m    562\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/miniconda3/envs/hrenv/lib/python3.12/site-packages/transformers/modeling_utils.py:3960\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3950\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3951\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3953\u001b[0m     (\n\u001b[1;32m   3954\u001b[0m         model,\n\u001b[1;32m   3955\u001b[0m         missing_keys,\n\u001b[1;32m   3956\u001b[0m         unexpected_keys,\n\u001b[1;32m   3957\u001b[0m         mismatched_keys,\n\u001b[1;32m   3958\u001b[0m         offload_index,\n\u001b[1;32m   3959\u001b[0m         error_msgs,\n\u001b[0;32m-> 3960\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3961\u001b[0m         model,\n\u001b[1;32m   3962\u001b[0m         state_dict,\n\u001b[1;32m   3963\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3964\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3965\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3966\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3967\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3968\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3969\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3970\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3971\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3972\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3973\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3974\u001b[0m         hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   3975\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3976\u001b[0m         gguf_path\u001b[39m=\u001b[39;49mgguf_path,\n\u001b[1;32m   3977\u001b[0m     )\n\u001b[1;32m   3979\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3980\u001b[0m model\u001b[39m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/hrenv/lib/python3.12/site-packages/transformers/modeling_utils.py:4434\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4430\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4431\u001b[0m                     model_to_load, key, \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mempty(\u001b[39m*\u001b[39mparam\u001b[39m.\u001b[39msize(), dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m   4432\u001b[0m                 )\n\u001b[1;32m   4433\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4434\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4435\u001b[0m             model_to_load,\n\u001b[1;32m   4436\u001b[0m             state_dict,\n\u001b[1;32m   4437\u001b[0m             loaded_keys,\n\u001b[1;32m   4438\u001b[0m             start_prefix,\n\u001b[1;32m   4439\u001b[0m             expected_keys,\n\u001b[1;32m   4440\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   4441\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   4442\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   4443\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   4444\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   4445\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   4446\u001b[0m             hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   4447\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   4448\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   4449\u001b[0m             unexpected_keys\u001b[39m=\u001b[39;49munexpected_keys,\n\u001b[1;32m   4450\u001b[0m         )\n\u001b[1;32m   4451\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   4452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4453\u001b[0m     \u001b[39m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hrenv/lib/python3.12/site-packages/transformers/modeling_utils.py:961\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    950\u001b[0m     state_dict_index \u001b[39m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    951\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    952\u001b[0m     \u001b[39mnot\u001b[39;00m is_quantized\n\u001b[1;32m    953\u001b[0m     \u001b[39mor\u001b[39;00m (\u001b[39mnot\u001b[39;00m hf_quantizer\u001b[39m.\u001b[39mrequires_parameters_quantization)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    959\u001b[0m ):\n\u001b[1;32m    960\u001b[0m     \u001b[39m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 961\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mset_module_kwargs)\n\u001b[1;32m    962\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     hf_quantizer\u001b[39m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/miniconda3/envs/hrenv/lib/python3.12/site-packages/accelerate/utils/modeling.py:416\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    414\u001b[0m             module\u001b[39m.\u001b[39m_parameters[tensor_name] \u001b[39m=\u001b[39m param_cls(new_value, requires_grad\u001b[39m=\u001b[39mold_value\u001b[39m.\u001b[39mrequires_grad)\n\u001b[1;32m    415\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 416\u001b[0m     new_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    417\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     new_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(value, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"../c10/cuda/CUDACachingAllocator.cpp\":838, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "# 1. 모델 및 토크나이저 로드\n",
    "model_id = os.path.join(CURR_PATH, config.model_path, config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "max_memory = {\n",
    "    0: \"28GB\",  # GPU 0 \n",
    "    'cpu': \"28GB\"  # CPU \n",
    "}\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    max_memory = max_memory \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 메모리 캐시 초기화\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# CUDA 캐싱 할당기에서 할당된 모든 메모리 해제\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/elicer/miniconda3/envs/hrenv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "\n",
      "WARNING:py.warnings:/home/elicer/miniconda3/envs/hrenv/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "\n",
      "WARNING:transformers_modules.Phi-3-medium-4k-instruct.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n",
      "WARNING:py.warnings:/home/elicer/.cache/huggingface/modules/transformers_modules/Phi-3-medium-4k-instruct/modeling_phi3.py:391: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "\n",
      "WARNING:py.warnings:/home/elicer/.cache/huggingface/modules/transformers_modules/Phi-3-medium-4k-instruct/modeling_phi3.py:398: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "\n",
      "WARNING:py.warnings:/home/elicer/.cache/huggingface/modules/transformers_modules/Phi-3-medium-4k-instruct/modeling_phi3.py:410: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ONNX 변환 시 사용할 더미 입력 데이터 생성\n",
    "# 시퀀스 길이는 모델에 맞게 조정할 수 있습니다 (예: 828)\n",
    "dummy_input = torch.randint(0, tokenizer.vocab_size, (1, 828), dtype=torch.long).to(\"cpu\")\n",
    "\n",
    "# 모델을 평가 모드로 설정 (Dropout 등 비활성화)\n",
    "llm.eval()\n",
    "\n",
    "# 모델을 CPU로 이동\n",
    "llm = llm.to('cpu')\n",
    "\n",
    "# ONNX 파일로 저장할 경로 설정\n",
    "onnx_model_path = \"./onnx/phi3-medium-4k-instruct-cuda-fp16.onnx\"\n",
    "\n",
    "# ONNX로 변환\n",
    "torch.onnx.export(\n",
    "    llm,                           # 변환할 모델\n",
    "    dummy_input,                   # 더미 입력 데이터\n",
    "    onnx_model_path,               # 저장할 ONNX 파일 경로\n",
    "    input_names=[\"input_ids\"],      # 입력 텐서 이름\n",
    "    output_names=[\"generated_outputs\"],        # 출력 텐서 이름\n",
    "    dynamic_axes={                  # 시퀀스 길이와 배치 크기를 동적으로 처리\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"generated_outputs\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "    },\n",
    "    opset_version=13,               # ONNX opset 버전 설정 (최신 PyTorch에서는 13 권장)\n",
    "    do_constant_folding=True,       # 상수 접기 최적화\n",
    ")\n",
    "\n",
    "print(f\"ONNX 모델이 {onnx_model_path}에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harikang",
   "language": "python",
   "name": "harikang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
