{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_tensorrt.dynamo.conversion.aten_ops_converters:Unable to import quantization op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer, \n",
    "                          Phi3ForCausalLM,\n",
    "                          Phi3Config,\n",
    "                          pipeline,\n",
    "                          )\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch_tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"TensorRT\")\n",
    "parser.add_argument(\"--model_path\", type=str, default=\"models/\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"Phi-3-medium-4k-instruct\")\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"data/\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"test_dataset.jsonl\")\n",
    "parser.add_argument('--seed',type=int, default=0)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "parser.add_argument('--dtype',type=str, default=\"auto\")\n",
    "parser.add_argument('--n', type=int, default=1)\n",
    "parser.add_argument('--temperature', type=float, default=0.0)\n",
    "\n",
    "config = parser.parse_args([])\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9026366ab946d9b7fd506ce0df47ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = os.path.join(config.model_path, config.model_name)\n",
    "\n",
    "model = Phi3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torchscript=True,\n",
    "    attn_implementation=\"eager\"\n",
    ").to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_text = \"Can you provide ways to eat combinations of bananas and dragonfruits?\"\n",
    "inputs = tokenizer(dummy_text, return_tensors=\"pt\")\n",
    "\n",
    "traced_model = torch.jit.trace(model, (inputs['input_ids'], inputs['attention_mask']))\n",
    "\n",
    "output = traced_model(inputs['input_ids'], inputs['attention_mask'], strict=False)\n",
    "print(output)\n",
    "\n",
    "# preds = tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enabled_precisions = {torch.half} # Run with FP16\n",
    "debug = True\n",
    "# workspace_size = 20 << 30\n",
    "min_block_size = 7  # Lower value allows more graph segmentation\n",
    "torch_executed_ops = {}\n",
    "\n",
    "compilation_args = {\n",
    "    \"enabled_precisions\": enabled_precisions,\n",
    "    \"debug\": debug,\n",
    "    # \"workspace_size\": workspace_size,\n",
    "    \"min_block_size\": min_block_size,\n",
    "    \"torch_executed_ops\": torch_executed_ops,\n",
    "}\n",
    "\n",
    "\n",
    "trt_model = torch_tensorrt.dynamo.compile(\n",
    "                        model, \n",
    "                        # mode=\"max-autotune\", \n",
    "                        backend=\"torch_tensorrt\",\n",
    "                        dynamic=False,\n",
    "                        fullgraph=True,\n",
    "                        options=compilation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model = torch.compile(model, \n",
    "                        backend=\"torch_tensorrt\",\n",
    "                        options={\n",
    "                            \"truncate_long_and_double\": True,\n",
    "                            \"enabled_precisions\": {torch.float16}\n",
    "                        },\n",
    "                        dynamic=False)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=config.batch_size,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "system_message = \"You are a helpful AI Assistant. Help users by replying to their queries and make sure the responses are polite. Do not hallucinate.\"\n",
    "PROMPT = f\"<|system|>\\n{system_message}<|end|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:599: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "\n",
    "data = load_dataset(\"json\", data_files=\"./data/test_dataset.jsonl\")['train']\n",
    "messages = list(KeyDataset(data,'message'))\n",
    "\n",
    "token_ids = tokenizer.apply_chat_template(messages, \n",
    "                                        add_generation_prompt=True, \n",
    "                                        tokenize=False,)\n",
    "                                        \n",
    "token_ids.insert(0, PROMPT)\n",
    "\n",
    "inputs = tokenizer(token_ids, return_tensors=\"pt\", padding=True)\n",
    "inputs = {k: v.type(torch.int32).to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outs = trt_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs.get(\"attention_mask\"),\n",
    "        **generation_args\n",
    "    )\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(outs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "processed_outs = [\n",
    "    [{\"generated_text\": text.rpartition('\\n')[2]}]\n",
    "    for text in generated_texts[1:]\n",
    "]\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Answers =====\n",
      "Deep sea animals\n",
      "is standard weight and size\n",
      "they are genetically called to\n",
      "south\n",
      "An aircraft taking a trip\n",
      "protozoa\n",
      "Green house\n",
      "it unfreezes, because it is cold-blooded\n",
      "It holds 500 mL of water\n",
      "the air becomes arid\n",
      "July\n",
      "speaking with a witness\n",
      "shell\n",
      "\n",
      "particles of iron\n",
      "H2O haze\n",
      "constellations to appear in one place in spring and another in fall\n",
      "glucose\n",
      "help prevent the effects of erosion\n",
      "wind\n",
      "salvage plastic bottles instead of throwing them away\n",
      "less energy used by the water heater\n",
      "people driving cars might be unaware the animal is close by\n",
      "light from our closest star\n",
      "the darkness is greatest\n",
      "Water\n",
      "clothing\n",
      "a cut peony\n",
      "an alligator's habitat\n",
      "has seeds outside the flesh, unlike the blueberry\n",
      "===== Perf result =====\n",
      "Elapsed_time:  2.990051779896021\n",
      "Correctness: 23/30\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Answers =====\")\n",
    "correct = 0\n",
    "for i, out in enumerate(processed_outs):\n",
    "    correct_answer = data[i][\"answer\"]\n",
    "    answer = out[0][\"generated_text\"].lstrip().replace(\"\\n\",\"\")\n",
    "    if answer == correct_answer:\n",
    "        correct += 1\n",
    "    print(answer)\n",
    " \n",
    "print(\"===== Perf result =====\")\n",
    "print(\"Elapsed_time: \", end-start)\n",
    "print(f\"Correctness: {correct}/{len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaikids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
