{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/modelopt/torch/quantization/tensor_quant.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  scaled_e4m3_abstract = torch.library.impl_abstract(\"trt::quantize_fp8\")(\n",
      "/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer, \n",
    "                          Phi3ForCausalLM,\n",
    "                          Phi3Config,\n",
    "                          pipeline,\n",
    "                          )\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch_tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"TensorRT\")\n",
    "parser.add_argument(\"--model_dir\", type=str, default=\"models/\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"Phi-3-medium-4k-instruct\")\n",
    "parser.add_argument(\"--data_dir\", type=str, default=\"data/\")\n",
    "parser.add_argument(\"--data_name\", type=str, default=\"test_dataset.jsonl\")\n",
    "parser.add_argument('--seed',type=int, default=0)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "parser.add_argument('--dtype',type=str, default=\"auto\")\n",
    "parser.add_argument('--n', type=int, default=1)\n",
    "parser.add_argument('--temperature', type=float, default=0.0)\n",
    "\n",
    "config = parser.parse_args([])\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859f843cc17f4c7bb26d984ed0a29a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = os.path.join(config.model_dir, config.model_name)\n",
    "\n",
    "# with torch.no_grad():\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"cuda\",\n",
    "        torch_dtype=torch.half,\n",
    "        trust_remote_code=True,\n",
    "        torchscript=True,\n",
    "        attn_implementation=\"eager\",\n",
    "        use_cache=False,\n",
    "    )\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/torch/nn/functional.py:2249: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert padding_idx < weight.size(0), \"Padding_idx must be within num_embeddings\"\n",
      "\n",
      "WARNING:py.warnings:/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:86: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "\n",
      "WARNING:py.warnings:/home/elicer/.cache/huggingface/modules/transformers_modules/Phi-3-medium-4k-instruct/modeling_phi3.py:391: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_message = \"Can you provide ways to eat combinations of bananas and dragonfruits?\"\n",
    "\n",
    "model_inputs = tokenizer(dummy_message, return_tensors=\"pt\")\n",
    "input_ids = model_inputs.input_ids.to(DEVICE)\n",
    "\n",
    "jit_model = torch.jit.trace(model, input_ids)\n",
    "\n",
    "# def export_llm(model, inputs, min_seq_len=1, max_seq_len=64):\n",
    "#     seq_len = torch.export.Dim(\"seq_len\", min=min_seq_len, max=max_seq_len)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         ep = torch.export.export(\n",
    "#             model, (inputs,), dynamic_shapes=({1: seq_len},), strict=False\n",
    "#         )\n",
    "#     return ep \n",
    "\n",
    "# model_ep = export_llm(model, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "# ]\n",
    "\n",
    "# # input_ids = tokenizer.apply_chat_template(dummy_messages, \n",
    "# #                                         add_generation_prompt=True, \n",
    "# #                                         tokenize=True,\n",
    "# #                                         return_tensors=\"pt\",\n",
    "# #                                         padding=True).to(DEVICE)\n",
    "\n",
    "# dummy_message = \"Can you provide ways to eat combinations of bananas and dragonfruits?\"\n",
    "# model_inputs = tokenizer(dummy_message, return_tensors=\"pt\", padding=True)\n",
    "# input_ids = model_inputs.input_ids\n",
    "# traced_model = torch.jit.trace(model.forward, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Input(shape=(1, 16), dtype=dtype.f16, format=memory_format.linear, domain=[0.0, 2.0))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [\n",
    "#     torch_tensorrt.Input(shape=[1, 16], dtype=torch.half),\n",
    "#     torch_tensorrt.Input(shape=[4, 16], dtype=torch.half),\n",
    "#     torch_tensorrt.Input(shape=[16, 16], dtype=torch.half),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.export.export(model, (input_ids, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_994/783606236.py:1: DeprecationWarning: Compiler option \"truncate_long_and_double\" is deprecated in favor of \"truncate_double\" as int64 is now natively supported, this option will be removed in the next version\n",
      "  trt_model = torch_tensorrt.dynamo.compile(\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Input graph should be an ExportedProgram but got type <class 'torch.jit._trace.TopLevelTracedModule'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/LLMInference/trt.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/trt.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trt_model \u001b[39m=\u001b[39m torch_tensorrt\u001b[39m.\u001b[39;49mdynamo\u001b[39m.\u001b[39;49mcompile(\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/trt.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     jit_model,\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/trt.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     inputs\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/trt.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     enabled_precisions\u001b[39m=\u001b[39;49m{torch\u001b[39m.\u001b[39;49mhalf},\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/trt.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     min_block_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/trt.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     truncate_long_and_double\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/trt.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     disable_t32\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/trt.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/trt.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m system_message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYou are a helpful AI Assistant. Help users by replying to their queries and make sure the responses are polite. Do not hallucinate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/trt.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m PROMPT \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<|assistant|>\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msystem_message\u001b[39m}\u001b[39;00m\u001b[39m<|end|>\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/yaikids/lib/python3.10/site-packages/torch_tensorrt/dynamo/_compiler.py:181\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(exported_program, inputs, device, disable_tf32, assume_dynamic_shape_support, sparse_weights, enabled_precisions, engine_capability, refit, debug, num_avg_timing_iters, workspace_size, dla_sram_size, dla_local_dram_size, dla_global_dram_size, truncate_double, require_full_compilation, min_block_size, torch_executed_ops, torch_executed_modules, pass_through_build_failures, max_aux_streams, version_compatible, optimization_level, use_python_runtime, use_fast_partitioner, enable_experimental_decompositions, dryrun, hardware_compatible, timing_cache_path, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m enabled_precisions \u001b[39m=\u001b[39m {dtype\u001b[39m.\u001b[39m_from(p) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m enabled_precisions}\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(exported_program, ExportedProgram):\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    182\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput graph should be an ExportedProgram but got type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(exported_program)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m exported_program \u001b[39m=\u001b[39m pre_export_lowering(exported_program, torch_inputs)\n\u001b[1;32m    185\u001b[0m exported_program \u001b[39m=\u001b[39m exported_program\u001b[39m.\u001b[39mrun_decompositions(\n\u001b[1;32m    186\u001b[0m     get_decompositions(enable_experimental_decompositions)\n\u001b[1;32m    187\u001b[0m )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Input graph should be an ExportedProgram but got type <class 'torch.jit._trace.TopLevelTracedModule'>"
     ]
    }
   ],
   "source": [
    "trt_model = torch_tensorrt.dynamo.compile(\n",
    "    jit_model,\n",
    "    inputs=input_ids,\n",
    "    enabled_precisions={torch.half},\n",
    "    min_block_size=1,\n",
    "    truncate_long_and_double=True,\n",
    "    disable_t32=True\n",
    ")\n",
    "\n",
    "system_message = \"You are a helpful AI Assistant. Help users by replying to their queries and make sure the responses are polite. Do not hallucinate.\"\n",
    "PROMPT = f\"<|assistant|>\\n{system_message}<|end|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(trt_model, 'trt.ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_message = \"Can you provide ways to eat combinations of bananas and dragonfruits?\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer(dummy_message, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "traced_model = torch.jit.trace(model, (input_ids[\"input_ids\"], input_ids[\"attention_mask\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/LLMInference/trt.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/trt.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/trt.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/trt.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m data \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m, data_files\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./data/test_dataset.jsonl\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "\n",
    "data = load_dataset(\"json\", data_files=\"./data/test_dataset.jsonl\")['train']\n",
    "messages = data['message']\n",
    "messages.insert(0, PROMPT)\n",
    "\n",
    "token_ids = tokenizer.apply_chat_template(messages, \n",
    "                                        add_generation_prompt=True, \n",
    "                                        tokenize=False,)\n",
    "\n",
    "toekn_ids.insert(0, PROMPT)\n",
    "\n",
    "inputs = tokenizer(toekn_ids, return_tensors=\"pt\", padding=True)\n",
    "inputs = {k: v.type(torch.int32).to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outs = trt_model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        **generation_args\n",
    "    )\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(outs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "results = [\n",
    "    [{\"generated_text\": text.rpartition('\\n')[2]}]\n",
    "    for text in generated_texts[1:]\n",
    "]\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== Answers =====\")\n",
    "correct = 0\n",
    "for i, out in enumerate(results):\n",
    "    correct_answer = data[i][\"answer\"]\n",
    "    answer = out[0][\"generated_text\"].lstrip().replace(\"\\n\",\"\")\n",
    "    if answer == correct_answer:\n",
    "        correct += 1\n",
    "    print(answer)\n",
    " \n",
    "print(\"===== Perf result =====\")\n",
    "print(\"Elapsed_time: \", end-start)\n",
    "print(f\"Correctness: {correct}/{len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaikids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
