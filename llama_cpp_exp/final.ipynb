{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/miniconda3/envs/jjj/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "###  Import Library  ###\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_speculative import LlamaPromptLookupDecoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Config  ###\n",
    "parser = argparse.ArgumentParser(description=\"llama.cpp\")\n",
    "parser.add_argument(\"--model_dir\", type=str, default=\"./Efficient-SLM-Inference/Phi-3-medium-4k-instruct-fp32-gguf\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"Phi-3-medium-4k-instruct-fp32.gguf\")\n",
    "parser.add_argument(\"--data_path\", type=str, default = \"./Efficient-SLM-Inference/data/test_dataset.jsonl\")\n",
    "parser.add_argument('--seed',type=int, default=0)\n",
    "parser.add_argument('--temperature', type=float, default=0.0)\n",
    "parser.add_argument('--verbose', type=bool, default=False)\n",
    "parser.add_argument('--n_gpu_layers', type=int, default=-1)\n",
    "parser.add_argument('--num_pred_tokens', type=int, default=10)\n",
    "\n",
    "config = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model path does not exist: ./Efficient-SLM-Inference/Phi-3-medium-4k-instruct-fp32-gguf/Phi-3-medium-4k-instruct-fp32.gguf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/Efficient-SLM-Inference/final.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ccdtxzcabztdkyds.tunnel-pt.elice.io/home/elicer/Efficient-SLM-Inference/final.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m _seed_everything(config\u001b[39m.\u001b[39mseed)\n\u001b[1;32m     <a href='vscode-notebook-cell://ccdtxzcabztdkyds.tunnel-pt.elice.io/home/elicer/Efficient-SLM-Inference/final.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m###  Load Model  ###\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ccdtxzcabztdkyds.tunnel-pt.elice.io/home/elicer/Efficient-SLM-Inference/final.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m model \u001b[39m=\u001b[39m Llama(\n\u001b[1;32m     <a href='vscode-notebook-cell://ccdtxzcabztdkyds.tunnel-pt.elice.io/home/elicer/Efficient-SLM-Inference/final.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \tmodel_path\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(config\u001b[39m.\u001b[39;49mmodel_dir, config\u001b[39m.\u001b[39;49mmodel_name),\n\u001b[1;32m     <a href='vscode-notebook-cell://ccdtxzcabztdkyds.tunnel-pt.elice.io/home/elicer/Efficient-SLM-Inference/final.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \tn_gpu_layers\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mn_gpu_layers,\n\u001b[1;32m     <a href='vscode-notebook-cell://ccdtxzcabztdkyds.tunnel-pt.elice.io/home/elicer/Efficient-SLM-Inference/final.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \tn_ctx\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ccdtxzcabztdkyds.tunnel-pt.elice.io/home/elicer/Efficient-SLM-Inference/final.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \tn_batch\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ccdtxzcabztdkyds.tunnel-pt.elice.io/home/elicer/Efficient-SLM-Inference/final.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \tverbose\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m     <a href='vscode-notebook-cell://ccdtxzcabztdkyds.tunnel-pt.elice.io/home/elicer/Efficient-SLM-Inference/final.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     seed\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mseed,\n\u001b[1;32m     <a href='vscode-notebook-cell://ccdtxzcabztdkyds.tunnel-pt.elice.io/home/elicer/Efficient-SLM-Inference/final.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \tdraft_model\u001b[39m=\u001b[39;49mLlamaPromptLookupDecoding(num_pred_tokens\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_pred_tokens),\n\u001b[1;32m     <a href='vscode-notebook-cell://ccdtxzcabztdkyds.tunnel-pt.elice.io/home/elicer/Efficient-SLM-Inference/final.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ccdtxzcabztdkyds.tunnel-pt.elice.io/home/elicer/Efficient-SLM-Inference/final.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m### Prompt  ###\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ccdtxzcabztdkyds.tunnel-pt.elice.io/home/elicer/Efficient-SLM-Inference/final.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m system_message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYou are a highly helpful and knowledgeable AI assistant specializing in answering user queries accurately and politely.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/jjj/lib/python3.10/site-packages/llama_cpp/llama.py:365\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, rpc_servers, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_ubatch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, spm_infill, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspm_infill \u001b[39m=\u001b[39m spm_infill\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(model_path):\n\u001b[0;32m--> 365\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel path does not exist: \u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    367\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stack\u001b[39m.\u001b[39menter_context(\n\u001b[1;32m    368\u001b[0m     contextlib\u001b[39m.\u001b[39mclosing(\n\u001b[1;32m    369\u001b[0m         internals\u001b[39m.\u001b[39mLlamaModel(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    375\u001b[0m )\n\u001b[1;32m    377\u001b[0m \u001b[39m# Override tokenizer\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Model path does not exist: ./Efficient-SLM-Inference/Phi-3-medium-4k-instruct-fp32-gguf/Phi-3-medium-4k-instruct-fp32.gguf"
     ]
    }
   ],
   "source": [
    "def _seed_everything(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True # False\n",
    "\n",
    "_seed_everything(config.seed)\n",
    "\n",
    "###  Load Model  ###\n",
    "model = Llama(\n",
    "\tmodel_path=os.path.join(config.model_dir, config.model_name),\n",
    "\tn_gpu_layers=config.n_gpu_layers,\n",
    "\tn_ctx=1024,\n",
    "\tn_batch=1024,\n",
    "\tverbose=config.verbose,\n",
    "    seed=config.seed,\n",
    "\tdraft_model=LlamaPromptLookupDecoding(num_pred_tokens=config.num_pred_tokens),\n",
    ")\n",
    "\n",
    "### Prompt  ###\n",
    "system_message = \"You are a highly helpful and knowledgeable AI assistant specializing in answering user queries accurately and politely.\"\n",
    "prompt = f\"<|system|>\\n{system_message}\\n\"\n",
    "\n",
    "def apply_chat_template(messages):\n",
    "    formatted_messages = []\n",
    "    \n",
    "    formatted_messages.extend(\n",
    "        ''.join(\n",
    "            [f\"{prompt}<|user|>\\n{msg.get('content', '').strip()}\\n<|end|>\\n\\n\" if msg.get('role') == 'user' else '' for msg in message]\n",
    "        ) + \"<|assistant|>\\n\" for message in messages\n",
    "    )\n",
    "    \n",
    "    return formatted_messages\n",
    "\n",
    "###  Warm up ###\n",
    "dummy = \"Can you provide ways to eat combinations of bananas and dragonfruits?\"\n",
    "output = model(\n",
    "      dummy,\n",
    "      max_tokens=32,\n",
    "      echo=False,\n",
    ")\n",
    "\n",
    "print(output['choices'][0]['text'])\n",
    "\n",
    "### Load data and Inference ### \n",
    "start = time.perf_counter()\n",
    "\n",
    "data = load_dataset(\"json\", data_files=config.data_path)['train']\n",
    "messages = data['message']\n",
    "token_ids = apply_chat_template(messages)\n",
    "\n",
    "outs = []\n",
    "for token_id in token_ids:\n",
    "    with torch.inference_mode(), torch.autocast(device_type=\"cuda\"):\n",
    "        output = model(token_id,\n",
    "        temperature=config.temperature,\n",
    "        echo=False)\n",
    "\n",
    "    out = output['choices'][0]['text']\n",
    "    \n",
    "    outs.append([{\n",
    "        'generated_text': out\n",
    "    }])\n",
    "\n",
    "end = time.perf_counter()\n",
    "            \n",
    "#### Benchmark ###\n",
    "print(\"===== Answers =====\")\n",
    "correct = 0\n",
    "for i, out in enumerate(outs):\n",
    "    correct_answer = data[i][\"answer\"]\n",
    "    answer = out[0][\"generated_text\"].lstrip().replace(\"\\n\",\"\")\n",
    "    if answer == correct_answer:\n",
    "        correct += 1\n",
    " \n",
    "print(\"===== Perf result =====\")\n",
    "print(\"Elapsed_time: \", end-start)\n",
    "print(f\"Correctness: {correct}/{len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jjj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
