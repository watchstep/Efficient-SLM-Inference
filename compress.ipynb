{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer, \n",
    "                          Phi3ForCausalLM,\n",
    "                          Phi3Config,\n",
    "                          pipeline,\n",
    "                          )\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "import utils\n",
    "\n",
    "config = utils.get_config()\n",
    "utils.seed_everything(config.seed)\n",
    "CURR_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Phi3LLMLinguaForCausalLM(Phi3ForCausalLM):\n",
    "#     def __init__(self, config:Phi3Config, tokenizer, compressor):\n",
    "#         super().__init__(config)\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.compressor = compressor\n",
    "    \n",
    "#     def generate(\n",
    "#         self,\n",
    "#         inputs: Optional[torch.Tensor] = None,\n",
    "#         generation_config: Optional[GenerationConfig] = None,\n",
    "#         logits_processor: Optional[LogitsProcessorList] = None,\n",
    "#         stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
    "#         prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
    "#         synced_gpus: Optional[bool] = None,\n",
    "#         assistant_model: Optional[\"PreTrainedModel\"] = None,\n",
    "#         streamer: Optional[\"BaseStreamer\"] = None,\n",
    "#         negative_prompt_ids: Optional[torch.Tensor] = None,\n",
    "#         negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n",
    "#         **kwargs,\n",
    "# )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28af4091a3b34a2ca4bac392befd8f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####### Section 1. Set up #######\n",
    "model_id = os.path.join(CURR_PATH, config.model_path, \"Phi-3-medium-4k-instruct\") # please replace with local model path\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    # attn_implementation=\"flash_attention_2\" # Running flash attention\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # batch_size=,\n",
    ")\n",
    " \n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To solve the equation 2x + 3 = 7, you need to isolate the variable x. Here's how you can do it step by step:\n",
      "\n",
      "1. Subtract 3 from both sides of the equation:\n",
      "   2x + 3 - 3 = 7 - 3\n",
      "   2x = 4\n",
      "\n",
      "2. Divide both sides of the equation by 2:\n",
      "   2x / 2 = 4 / 2\n",
      "   x = 2\n",
      "\n",
      "So, the solution to the equation 2x + 3 = 7 is x = 2.\n"
     ]
    }
   ],
   "source": [
    "####### Section 2. GPU Warm up #######\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Section 3. Prompt Compression #######\n",
    "import json\n",
    "import re\n",
    "from llmlingua import PromptCompressor\n",
    "\n",
    "COMPRESSOR_MAP = {\n",
    "   \"gpt2\" : \"lgaalves/gpt2-dolly\",\n",
    "   \"phi2\": \"microsoft/phi-2\",\n",
    "   \"llama2\": \"TheBloke/Llama-2-7b-Chat-GPTQ\",\n",
    "   \"roberta\" : \"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\",\n",
    "}\n",
    "compressor_id = COMPRESSOR_MAP[\"roberta\"]\n",
    "\n",
    "llm_lingua = PromptCompressor(compressor_id, device_map=\"auto\", use_llmlingua2=True)\n",
    "\n",
    "# def compress(content):\n",
    "#     data = content.split('\\n\\n')\n",
    "#     instruction = data[0]\n",
    "#     question = data[1].split('\\n')[0]\n",
    "#     choice = data[1].split('\\n')[1]\n",
    "#     compressed_prompt = llm_lingua.compress_prompt(\n",
    "#         context=[instruction, question],\n",
    "#         instruction=\"\",\n",
    "#         question=\"\",\n",
    "#         rate=0.75,\n",
    "#         iterative_size=100,\n",
    "#         context_budget=\"*2\",\n",
    "#     )\n",
    "#     return compressed_prompt['compressed_prompt'] + '\\n' + choice\n",
    "\n",
    "def compress(content):\n",
    "    data = content.split('\\n\\n')\n",
    "    instruction = '\\n' + data[0]\n",
    "    ddata = data[1].split('\\n')\n",
    "    question = ddata[0]\n",
    "    choice = ddata[1]\n",
    "    \n",
    "    compressed_prompt = llm_lingua.compress_prompt(\n",
    "        context=\"\\n\".join([instruction, question]),\n",
    "        rate=0.7,\n",
    "        force_tokens=[\"\\n\", \"?\", \".\", \",\" \"question:\"],\n",
    "    )\n",
    "\n",
    "    compressed_prompt = re.sub(r'\\n\\s+', '\\n', compressed_prompt['compressed_prompt'])\n",
    "    # .replace('question:', '\\nquestion:')\n",
    "    return f\"{compressed_prompt}\\n{choice}\\n\"\n",
    "    \n",
    "def compressed_jsonl(input_file_path, output_file_path):\n",
    "    with open(input_file_path, 'r') as reader:\n",
    "        with open(output_file_path, 'w') as writer:\n",
    "            for line in reader:\n",
    "                line = json.loads(line)\n",
    "                line[\"message\"][0][\"content\"] = compress(line[\"message\"][0][\"content\"])\n",
    "                \n",
    "                json.dump(line, writer)\n",
    "                writer.write(\"\\n\")\n",
    "\n",
    "input_file_path = os.path.join(CURR_PATH, config.data_path, \"test_dataset.jsonl\")\n",
    "output_file_path = os.path.join(CURR_PATH, config.data_path, \"test_dataset_compressed.jsonl\")\n",
    "compressed_jsonl(input_file_path, output_file_path)\n",
    "# data.map(lambda x: print(x['message'][0]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d856339fcdd04e0d8f882242c14d88b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####### Section 3. Load data and Inference -> Performance evaluation part #######\n",
    "start = time.time()\n",
    "# data = load_dataset(\"json\", data_files=input_file_path)['train']\n",
    "data = load_dataset(\"json\", data_files=output_file_path)['train']\n",
    "outs = pipe(KeyDataset(data, 'message'), **generation_args)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Answers =====\n",
      "Deep sea animals\n",
      "uses what it needs\n",
      "they are genetically called to\n",
      "south\n",
      "A snail moving across the sidewalk\n",
      "protozoa\n",
      "Green house\n",
      "it unfreezes, because it is cold-blooded\n",
      "It is a sphere\n",
      "fluid spreads from pores\n",
      "July\n",
      "speaking with a witness\n",
      "shell\n",
      "the final barrel is gone, there supply is finished\n",
      "particles of iron\n",
      "H2O haze\n",
      "constellations to appear in one place in spring and another in fall\n",
      "glucose\n",
      "help prevent the effects of erosion\n",
      "wind\n",
      "salvage plastic bottles instead of throwing them away\n",
      "less energy used by the water heater\n",
      "people driving cars might be unaware the animal is close by\n",
      "light from our closest star\n",
      "the darkness is greatest\n",
      "Water\n",
      "clothing\n",
      "a cut peony\n",
      "an alligator's habitat\n",
      "has seeds outside the flesh, unlike the blueberry\n",
      "===== Perf result =====\n",
      "Elapsed_time:  9.633576154708862\n",
      "Correctness: 24/30\n"
     ]
    }
   ],
   "source": [
    "####### Section 4. Accuracy (Just for leasderboard) #######\n",
    "print(\"===== Answers =====\")\n",
    "correct = 0\n",
    "for i, out in enumerate(outs):\n",
    "    correct_answer = data[i][\"answer\"]\n",
    "    answer = out[0][\"generated_text\"].lstrip().replace(\"\\n\",\"\")\n",
    "    if answer == correct_answer:\n",
    "        correct += 1\n",
    "    print(answer)\n",
    " \n",
    "print(\"===== Perf result =====\")\n",
    "print(\"Elapsed_time: \", end-start)\n",
    "print(f\"Correctness: {correct}/{len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaikids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
