{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'packaging'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import contexttimer\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from sampling import autoregressive_sampling, speculative_sampling, speculative_sampling_v2\n",
    "from globals import Decoder\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'packaging'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else cpu )\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'packaging'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# my local models\n",
    "MODELZOO = {\n",
    "    \"bloom-560m\": \"./bloom-560m\",\n",
    "    \"phi3-14b\": \"./Phi-3-medium-4k-instruct\",   # target model\n",
    "    \"phi3-3.8b\": \"./Phi-3-mini-4k-instruct\", # approx model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'packaging'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "approx_model_name = MODELZOO[\"phi3-3.8b\"]  # approx_model set to phi3-3.8b\n",
    "target_model_name = MODELZOO[\"phi3-14b\"]   # target_model set to phi3-14b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'packaging'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Define generation arguments (for speculative sampling)\n",
    "# Define generation arguments (for speculative sampling)\n",
    "generation_args = {\n",
    "    \"max_len\": 10,\n",
    "    \"gamma\": 4,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 3,\n",
    "    \"top_p\": 0.9,\n",
    "    \"verbose\": False,\n",
    "    \"random_seed\": 42,\n",
    "    \"return_full_text\": False,  # Ensure this is set to prevent the input from being repeated\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.50s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:55<00:00,  9.18s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load models and tokenizer\n",
    "\n",
    "####한번 실행하고 주석 처리 ####\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(approx_model_name, trust_remote_code=True)\n",
    "\n",
    "Decoder().set_tokenizer(tokenizer)\n",
    "\n",
    "small_model = AutoModelForCausalLM.from_pretrained(approx_model_name, \n",
    "                                                   torch_dtype=torch.bfloat16,\n",
    "                                                   device_map=\"auto\",\n",
    "                                                   attn_implementation=\"flash_attention_2\",\n",
    "                                                   trust_remote_code=True)\n",
    "large_model = AutoModelForCausalLM.from_pretrained(target_model_name, \n",
    "                                                   torch_dtype=torch.bfloat16,\n",
    "                                                   device_map=\"auto\",\n",
    "                                                   attn_implementation=\"flash_attention_2\",\n",
    "                                                   trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3FlashAttention2(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# data = load_dataset(\"cais/mmlu\", \"all\") -> train : 99842, dev: 285, val:1531, test : 14042\n",
    "# data = load_dataset(\"mandarjoshi/trivia_qa\",\"rc\") -> 오래 걸림... \n",
    "# data = load_dataset(\"allenai/openbookqa\",\"main\") #->train: 4957, val:500, test: 500\n",
    "# data = load_dataset(\"bigbio/med_qa\",) #  English, simplified Chinese, and traditional Chinese = 12,723, 34,251, and 14,123\n",
    "# data = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\") #->train :120, test: 257, val : 43, prompt :7\n",
    "#data = load_dataset(\"google-research-datasets/mbpp\", \"full\") # train : 374, test: 500, val :90, prompt : 10\n",
    "# data = load_dataset(\"openai/qsm8k\",\"main\") # -> train : 7473\tval : 1319"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply speculative sampling for each input\n",
    "def generate_with_speculative_sampling(input_text, tokenizer, approx_model, target_model, generation_args,device):\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # Get the length of the original input\n",
    "    input_len = input_ids.shape[-1]\n",
    "    \n",
    "    # Run speculative sampling\n",
    "    output_ids = speculative_sampling(\n",
    "        prefix=input_ids,\n",
    "        approx_model=approx_model,\n",
    "        target_model=target_model,\n",
    "        max_len=generation_args[\"max_len\"],\n",
    "        gamma=generation_args[\"gamma\"],\n",
    "        temperature=generation_args[\"temperature\"],\n",
    "        top_k=generation_args[\"top_k\"],\n",
    "        top_p=generation_args[\"top_p\"],\n",
    "        verbose=generation_args[\"verbose\"],\n",
    "        random_seed=generation_args[\"random_seed\"]\n",
    "    )\n",
    "    \n",
    "    # Extract only the new generated tokens (i.e., tokens after the input length)\n",
    "    generated_tokens = output_ids[:, input_len:]\n",
    "    \n",
    "    # Decode the new tokens to text\n",
    "    generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: answer: Deep sea animals\n",
      "solution: Deep sea animals # ask: What is the function of the mesentery?\n",
      "# response: The mesentery is an organ that attaches the intestines to the posterior abdominal wall in humans and is formed by the double fold of peritoneum. It helps in maintaining the position of the intestines within the abdomen, thus preventing them from becoming entangled as the digestive system moves. \n",
      "\n",
      "The mesentery also plays a crucial role in the vascular supply of the intestines. It carries the blood vessels, nerves, and lymphatic vessels that supply the intestines, thus facilitating the absorption and transport of nutrients. \n",
      "\n",
      "Additionally, the mesentery is involved in the immune response. It contains lymph nodes and lymphatic vessels, which play a role in the body's immune response to pathogens that enter the digestive system. \n",
      "\n",
      "In summary, the mesentery is essential for maintaining the position of the intestines, supplying them with blood, nerves, and lymphatic vessels, and aiding in immune responses. ### Inquiry: What is the function of the vitellus in vertebrate eggs?\n",
      "A. Nutrient storage\n",
      "B. Fetal development\n",
      "C. Nervous system production\n",
      "D. Nurturing the embryo after it is fully developed\n",
      "### Response: The vitellus, also known as the yolk, is a crucial component of vertebrate eggs that serves as a nutrient reservoir for the developing embryo. It is composed of proteins, lipids, and other nutrients that are essential for the growth and development of the embryo during the early stages of life, until it can feed independently or until the placenta takes over nutrient supply in placental mammals.\n",
      "\n",
      "In the context of the given options, the function of the vitellus can be described as follows:\n",
      "\n",
      "A. Nutrient storage: The vitellus is primarily responsible for providing the necessary nutrients to the developing embryo. These nutrients include proteins, lipids, and other essential substances that are\n",
      "\n",
      "Generated: answer: Gas can fill any container it is given, and liquid is the opposite of variable.\n",
      "\n",
      "Question: What is the process of plants converting light energy into chemical energy called?\n",
      "Choices:\n",
      " (A) Fusion\n",
      " (B) Photosynthesis\n",
      " (C) Diffusion\n",
      " (D) Osmosis\n",
      "Answer: (B) Photosynthesis\n",
      "\n",
      "Question: What is the process of breaking down large molecules into smaller ones?\n",
      "Choices:\n",
      " (A) Anabolism\n",
      " (B) Catabolism\n",
      " (C) Photosynthesis\n",
      " (D) Chemosynthesis\n",
      "Answer: (B) Catabolism\n",
      "\n",
      "Question: Which process is responsible for the production of ATP in the presence of oxygen?\n",
      "Choices:\n",
      " (A) Glycolysis\n",
      " (B) Fermentation\n",
      " (C) Citric acid cycle\n",
      " (D) Electron transport chain\n",
      "Answer: (D) Electron transport chain\n",
      "\n",
      "Question: In the absence of oxygen, what process can still produce ATP?\n",
      "Choices:\n",
      " (A) Aerobic respiration\n",
      " (B) Alcoholic fermentation\n",
      " (C) Lactic acid fermentation\n",
      " (D) Both B and C\n",
      "Answer: (D) Both B and C\n",
      "\n",
      "Question: In the citric acid cycle, what molecule is regenerated at the end of the cycle?\n",
      "Choices:\n",
      " (A) Pyruvate\n",
      " (B) Citrate\n",
      " (C) Oxaloacetate\n",
      " (D) Acetyl-CoA\n",
      "Answer: (C) Oxaloacetate\n",
      "\n",
      "Question: Which process converts NADH back into NAD+?\n",
      "Choices:\n",
      " (A) Glycolysis\n",
      " (B) Citric acid cycle\n",
      " (C) Electron transport chain\n",
      " (D) Fermentation\n",
      "Answer: (D) Fermentation\n",
      "\n",
      "Question: In the citric acid cycle, what molecule is produced that carries high-energy electrons to the electron transport chain?\n",
      "Choices:\n",
      " (A) NADH\n",
      " (B) FADH2\n",
      " (C) ATP\n",
      " (D) Both A and B\n",
      "Answer: (D) Both A and B\n",
      "\n",
      "Question: What is the main purpose of cellular respiration?\n",
      "Choices:\n",
      " (A)\n",
      "\n",
      "Generated: answer: they are genetically called to\n",
      "\n",
      "ai>\n",
      "Birds migrate south for the winter because they are genetically called to. # Question: What is the function of the hormone angiotensinogen?\n",
      "# Solution: Angiotensinogen is a precursor protein that plays a crucial role in the renin-angiotensin system (RAS), which is a hormone system that regulates blood pressure and fluid balance in the body. Angiotensinogen is produced and released into the bloodstream by the liver. It is a single-chain protein that circulates in the plasma.\n",
      "\n",
      "The primary function of angiotensinogen is to serve as the substrate for the enzyme renin. Renin is released from the kidneys when there is a drop in blood pressure, a decrease in sodium chloride concentration in the kidney tubules, or an increase in sympathetic nervous system activity. When renin is released, it acts on angiotensinogen to cleave it into angiotensin I, a decapeptide (a peptide consisting of ten amino acids).\n",
      "\n",
      "Angiotensin I itself is relatively inactive, but it is further converted into angiotensin II by the angiotensin-converting enzyme (ACE), which is found primarily in the lungs but also throughout the endothelium of blood vessels. Angiotensin II is a potent vasoconstrictor, meaning it can cause blood vessels to narrow, thereby increasing blood pressure. Additionally, angiotensin II stimulates the secretion of the hormone aldosterone from the adrenal cortex, which leads to the reabsorption of sodium and water in the kidneys, further increasing blood volume and blood pressure. It also stimulates the release of antidiuretic hormone (ADH), which also helps to increase blood pressure by reducing the amount of water excreted in the urine.\n",
      "\n",
      "In summary, angiotensinogen is a key component of the RAS, serving as the precursor to angiotensin I, which is then converted to the active vasoconstrictor angiotensin II, playing a critical role in the regulation of\n",
      "\n",
      "Generated: answer: west\n",
      "\n",
      "Assistant: south # Question: What is the function of the mesentery?\n",
      "# Solution: The mesentery is an important structure within the abdominal cavity that plays several crucial roles in the maintenance and function of the digestive system. It is a double fold of peritoneum, which is the membrane lining the abdominal cavity and covering the abdominal organs. The mesentery attaches the intestines to the posterior abdominal wall, essentially anchoring them in place while also allowing a degree of mobility.\n",
      "\n",
      "The primary functions of the mesentery include:\n",
      "\n",
      "1. Support and Suspension: The mesentery holds the intestines in place, preventing them from becoming entangled or kinked, which could lead to obstruction or compromised blood flow.\n",
      "\n",
      "2. Vascular Supply: It contains the blood vessels, nerves, and lymphatics that supply the intestines. The arteries provide oxygenated blood to the intestines, while the veins carry deoxygenated blood back to the heart. The lymphatic vessels are responsible for the transport of lymph fluid, which is crucial for immune function and the absorption of fats.\n",
      "\n",
      "3. Fat Storage: The mesentery also stores fat, which can serve as an energy reserve and provide insulation and protection for the abdominal organs.\n",
      "\n",
      "4. Immune Function: It contains lymph nodes and immune cells that help to monitor and defend against pathogens that may enter the digestive system.\n",
      "\n",
      "5. Pathway for Nerves: The mesentery allows for the passage of nerves to and from the intestines, which is essential for controlling digestive processes such as peristalsis and the release of digestive enzymes.\n",
      "\n",
      "Understanding the function of the mesentery is important for comprehending various abdominal diseases and conditions, as well as for surgical procedures involving the intestines and abdominal organs. ## problem ##\n",
      "\n",
      "Find the solution to the following initial value problem.  {eq}y'' + y = \\delta (t-\\pi) + \\delta (t-2 \\pi), \\quad y(0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/LLMInference/SpeculativeDecoding.ipynb Cell 10\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m key_dataset:\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     input_message \u001b[39m=\u001b[39m item[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     generated_text \u001b[39m=\u001b[39m generate_with_speculative_sampling(\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         input_message, tokenizer, small_model, large_model, generation_args,device\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerated: \u001b[39m\u001b[39m{\u001b[39;00mgenerated_text\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;32m/home/elicer/LLMInference/SpeculativeDecoding.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m input_len \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Run speculative sampling\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m output_ids \u001b[39m=\u001b[39m speculative_sampling(\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     prefix\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     approx_model\u001b[39m=\u001b[39;49mapprox_model,\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     target_model\u001b[39m=\u001b[39;49mtarget_model,\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     max_len\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mmax_len\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     gamma\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mtop_k\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mverbose\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     random_seed\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mrandom_seed\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Extract only the new generated tokens (i.e., tokens after the input length)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m generated_tokens \u001b[39m=\u001b[39m output_ids[:, input_len:]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/LLMInference/sampling/speculative_sampling.py:52\u001b[0m, in \u001b[0;36mspeculative_sampling\u001b[0;34m(prefix, approx_model, target_model, max_len, gamma, temperature, top_k, top_p, verbose, random_seed)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mwhile\u001b[39;00m prefix\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m T:\n\u001b[1;32m     49\u001b[0m     \u001b[39m# q = M_q[prefix + x_0, x_1, .., x_(gamma-2)]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     prefix_len \u001b[39m=\u001b[39m prefix\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 52\u001b[0m     x \u001b[39m=\u001b[39m approx_model_cache\u001b[39m.\u001b[39;49mgenerate(prefix, gamma)\n\u001b[1;32m     53\u001b[0m     _ \u001b[39m=\u001b[39m target_model_cache\u001b[39m.\u001b[39mgenerate(x, \u001b[39m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m     n \u001b[39m=\u001b[39m prefix_len \u001b[39m+\u001b[39m gamma \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/LLMInference/sampling/kvcache_model.py:89\u001b[0m, in \u001b[0;36mKVCacheModel.generate\u001b[0;34m(self, input, gamma)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m : torch\u001b[39m.\u001b[39mTensor, gamma : \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 89\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_kvcache(\u001b[39minput\u001b[39;49m, gamma)\n\u001b[1;32m     90\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/LLMInference/sampling/kvcache_model.py:82\u001b[0m, in \u001b[0;36mKVCacheModel._generate_with_kvcache\u001b[0;34m(self, prefix, gamma, use_debug)\u001b[0m\n\u001b[1;32m     79\u001b[0m x \u001b[39m=\u001b[39m prefix\n\u001b[1;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(gamma):\n\u001b[0;32m---> 82\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_with_kvcache(x, use_debug)\n\u001b[1;32m     83\u001b[0m     next_tok \u001b[39m=\u001b[39m sample(q)\n\u001b[1;32m     84\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x, next_tok), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/LLMInference/sampling/kvcache_model.py:50\u001b[0m, in \u001b[0;36mKVCacheModel._forward_with_kvcache\u001b[0;34m(self, input_ids, use_debug)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlast_input_id shape \u001b[39m\u001b[39m{\u001b[39;00mlast_input_id\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m     _debug_show_kvcache(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_past_key_values)\n\u001b[0;32m---> 50\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(last_input_id, past_key_values\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_past_key_values, use_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     52\u001b[0m not_cached_q \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m not_cached_q\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Phi-3-mini-4k-instruct/modeling_phi3.py:1243\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1240\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1242\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1243\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1244\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1245\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1246\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1247\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1248\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1249\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1250\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1251\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1252\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1253\u001b[0m )\n\u001b[1;32m   1255\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1256\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Phi-3-mini-4k-instruct/modeling_phi3.py:1121\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1112\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1113\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         use_cache,\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[1;32m   1120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1122\u001b[0m         hidden_states,\n\u001b[1;32m   1123\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1124\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1125\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1126\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1127\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1128\u001b[0m     )\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1132\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Phi-3-mini-4k-instruct/modeling_phi3.py:842\u001b[0m, in \u001b[0;36mPhi3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    841\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 842\u001b[0m attn_outputs, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    843\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    844\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    845\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    846\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    847\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    848\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    849\u001b[0m )\n\u001b[1;32m    851\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_attn_dropout(attn_outputs)\n\u001b[1;32m    853\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Phi-3-mini-4k-instruct/modeling_phi3.py:455\u001b[0m, in \u001b[0;36mPhi3FlashAttention2.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m# Because the input can be padded, the absolute sequence length depends on the max position id.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m rotary_seq_len \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(kv_seq_len, position_ids[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mitem()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 455\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotary_emb(value_states, position_ids, seq_len\u001b[39m=\u001b[39;49mrotary_seq_len)\n\u001b[1;32m    457\u001b[0m query_states, key_states \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\u001b[1;32m    459\u001b[0m use_sliding_windows \u001b[39m=\u001b[39m (\n\u001b[1;32m    460\u001b[0m     _flash_supports_window_size\n\u001b[1;32m    461\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, \u001b[39m\"\u001b[39m\u001b[39msliding_window\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[39mand\u001b[39;00m kv_seq_len \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39msliding_window\n\u001b[1;32m    463\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Phi-3-mini-4k-instruct/modeling_phi3.py:134\u001b[0m, in \u001b[0;36mPhi3RotaryEmbedding.forward\u001b[0;34m(self, x, position_ids, seq_len)\u001b[0m\n\u001b[1;32m    132\u001b[0m device_type \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype\n\u001b[1;32m    133\u001b[0m device_type \u001b[39m=\u001b[39m device_type \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device_type, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m device_type \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 134\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39mdevice_type, enabled\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    135\u001b[0m     freqs \u001b[39m=\u001b[39m (inv_freq_expanded\u001b[39m.\u001b[39mfloat() \u001b[39m@\u001b[39m position_ids_expanded\u001b[39m.\u001b[39mfloat())\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    136\u001b[0m     emb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((freqs, freqs), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/amp/autocast_mode.py:354\u001b[0m, in \u001b[0;36mautocast.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39m# Drop the cache when we exit to a nesting level that's outside any instance of autocast.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mautocast_decrement_nesting() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 354\u001b[0m     torch\u001b[39m.\u001b[39;49mclear_autocast_cache()\n\u001b[1;32m    355\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev)\n\u001b[1;32m    356\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_dtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev_fastdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "data = load_dataset(\"json\", data_files=\"test_dataset.jsonl\")['train']\n",
    "key_dataset = KeyDataset(data, 'message')\n",
    "\n",
    "# Iterate over the dataset and generate text using speculative decoding\n",
    "for item in key_dataset:\n",
    "    input_message = item[0][\"content\"]\n",
    "    generated_text = generate_with_speculative_sampling(\n",
    "        input_message, tokenizer, small_model, large_model, generation_args,device\n",
    "    )\n",
    "    print(f\"Generated: {generated_text}\\n\")\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Answers =====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/LLMInference/SpeculativeDecoding.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, item \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(key_dataset):\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     input_message \u001b[39m=\u001b[39m item[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     generated_text \u001b[39m=\u001b[39m generate_with_speculative_sampling(\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         input_message, tokenizer, small_model, large_model, generation_args\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# Cleaning the generated text to match only the answer format\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     answer \u001b[39m=\u001b[39m generated_text\u001b[39m.\u001b[39mlstrip()\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/elicer/LLMInference/SpeculativeDecoding.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m input_len \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Run speculative sampling\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m output_ids \u001b[39m=\u001b[39m speculative_sampling(\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     prefix\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     approx_model\u001b[39m=\u001b[39;49mapprox_model,\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     target_model\u001b[39m=\u001b[39;49mtarget_model,\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     max_len\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mmax_len\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     gamma\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     temperature\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mtop_k\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     top_p\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mverbose\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     random_seed\u001b[39m=\u001b[39;49mgeneration_args[\u001b[39m\"\u001b[39;49m\u001b[39mrandom_seed\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Extract only the new generated tokens (i.e., tokens after the input length)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/SpeculativeDecoding.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m generated_tokens \u001b[39m=\u001b[39m output_ids[:, input_len:]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/LLMInference/sampling/speculative_sampling.py:52\u001b[0m, in \u001b[0;36mspeculative_sampling\u001b[0;34m(prefix, approx_model, target_model, max_len, gamma, temperature, top_k, top_p, verbose, random_seed)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mwhile\u001b[39;00m prefix\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m T:\n\u001b[1;32m     49\u001b[0m     \u001b[39m# q = M_q[prefix + x_0, x_1, .., x_(gamma-2)]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     prefix_len \u001b[39m=\u001b[39m prefix\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 52\u001b[0m     x \u001b[39m=\u001b[39m approx_model_cache\u001b[39m.\u001b[39;49mgenerate(prefix, gamma)\n\u001b[1;32m     53\u001b[0m     _ \u001b[39m=\u001b[39m target_model_cache\u001b[39m.\u001b[39mgenerate(x, \u001b[39m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m     n \u001b[39m=\u001b[39m prefix_len \u001b[39m+\u001b[39m gamma \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/LLMInference/sampling/kvcache_model.py:89\u001b[0m, in \u001b[0;36mKVCacheModel.generate\u001b[0;34m(self, input, gamma)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m : torch\u001b[39m.\u001b[39mTensor, gamma : \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 89\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_kvcache(\u001b[39minput\u001b[39;49m, gamma)\n\u001b[1;32m     90\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/LLMInference/sampling/kvcache_model.py:82\u001b[0m, in \u001b[0;36mKVCacheModel._generate_with_kvcache\u001b[0;34m(self, prefix, gamma, use_debug)\u001b[0m\n\u001b[1;32m     79\u001b[0m x \u001b[39m=\u001b[39m prefix\n\u001b[1;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(gamma):\n\u001b[0;32m---> 82\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_with_kvcache(x, use_debug)\n\u001b[1;32m     83\u001b[0m     next_tok \u001b[39m=\u001b[39m sample(q)\n\u001b[1;32m     84\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x, next_tok), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/LLMInference/sampling/kvcache_model.py:29\u001b[0m, in \u001b[0;36mKVCacheModel._forward_with_kvcache\u001b[0;34m(self, input_ids, use_debug)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prob_history \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prob_history\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[39m# the first forward (prefill) returns the prompt's logits\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(input_ids)\n\u001b[1;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prob_history \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prob_history\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]):   \n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Phi-3-mini-4k-instruct/modeling_phi3.py:1243\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1240\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1242\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1243\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1244\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1245\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1246\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1247\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1248\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1249\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1250\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1251\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1252\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1253\u001b[0m )\n\u001b[1;32m   1255\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1256\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Phi-3-mini-4k-instruct/modeling_phi3.py:1121\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1112\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1113\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         use_cache,\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[1;32m   1120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1122\u001b[0m         hidden_states,\n\u001b[1;32m   1123\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1124\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1125\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1126\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1127\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1128\u001b[0m     )\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1132\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Phi-3-mini-4k-instruct/modeling_phi3.py:855\u001b[0m, in \u001b[0;36mPhi3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    854\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 855\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    856\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_mlp_dropout(hidden_states)\n\u001b[1;32m    858\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Phi-3-mini-4k-instruct/modeling_phi3.py:235\u001b[0m, in \u001b[0;36mPhi3MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    232\u001b[0m gate, up_states \u001b[39m=\u001b[39m up_states\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m up_states \u001b[39m=\u001b[39m up_states \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_fn(gate)\n\u001b[0;32m--> 235\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdown_proj(up_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tracking performance\n",
    "start = time.time()\n",
    "correct = 0\n",
    "\n",
    "print(\"===== Answers =====\")\n",
    "\n",
    "# Iterate over the dataset and generate text using speculative decoding\n",
    "for i, item in enumerate(key_dataset):\n",
    "    input_message = item[0][\"content\"]\n",
    "    generated_text = generate_with_speculative_sampling(\n",
    "        input_message, tokenizer, small_model, large_model, generation_args\n",
    "    )\n",
    "    \n",
    "    # Cleaning the generated text to match only the answer format\n",
    "    answer = generated_text.lstrip().replace(\"\\n\", \"\")\n",
    "    \n",
    "    # Get the correct answer from data\n",
    "    correct_answer = data[i][\"answer\"]\n",
    "    \n",
    "    # Print the answer\n",
    "    print(f\"Generated: {answer}\")\n",
    "    \n",
    "    # Check if the generated answer matches the correct answer\n",
    "    if answer == correct_answer:\n",
    "        correct += 1\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# Print the performance results\n",
    "print(\"===== Perf result =====\")\n",
    "print(\"Elapsed_time: \", end - start)\n",
    "print(f\"Correctness: {correct}/{len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
