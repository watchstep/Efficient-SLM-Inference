{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_tensorrt.dynamo.conversion.aten_ops_converters:Unable to import quantization op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import torch_tensorrt\n",
    "\n",
    "from pytriton.model_config import ModelConfig, Tensor\n",
    "from pytriton.proxy.types import Request\n",
    "from pytriton.triton import Triton, TritonConfig\n",
    "from pytriton.decorators import batch\n",
    "import torch_tensorrt\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi3Deployment:\n",
    "    def __init__(self, model_id):\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torchscript=True,\n",
    "            attn_implementation=\"eager\")\n",
    "        \n",
    "        self.trt_model = torch.compile(\n",
    "                            model,\n",
    "                            backend=\"torch_tensorrt\",\n",
    "                            options={\n",
    "                                \"truncate_long_and_double\": True,\n",
    "                                \"enabled_precisions\": {torch.float16}\n",
    "                                },\n",
    "                            dynamic=False)\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, padding_size='left')\n",
    "    \n",
    "    def infer_fn(self, request):\n",
    "        generation_args = {\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "        }\n",
    "\n",
    "        system_message = \"You are a helpful AI Assistant. Help users by replying to their queries and make sure the responses are polite. Do not hallucinate.\"\n",
    "        PROMPT = f\"<|system|>\\n{system_message}<|end|>\"\n",
    "        \n",
    "        msg = request[0].values()\n",
    "        msgs = request.tobytes().decode(\"utf-8\")\n",
    "        token_ids = self.tokenizer.apply_chat_template(msgs, \n",
    "                                                add_generation_prompt=True, \n",
    "                                                tokenize=False,)\n",
    "        print(token_ids)\n",
    "\n",
    "        if type(token_ids) is list:                                     \n",
    "            token_ids.insert(0, PROMPT)\n",
    "        else:\n",
    "            token_ids = [PROMPT] + [token_ids]\n",
    "        \n",
    "        inputs = self.tokenizer(token_ids, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.type(torch.int32).to(DEVICE) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outs = self.trt_model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs.get(\"attention_mask\"),\n",
    "                **generation_args)\n",
    "\n",
    "        generated_texts = self.tokenizer.batch_decode(outs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "        json_texts = json.dumps(generated_texts)\n",
    "        np_results = np.frombuffer(json_str.encode('utf-8'), dtype=np.uint8)\n",
    "\n",
    "        return np_results\n",
    "        # async for text in generated_texts[1:]:\n",
    "        #     result = np.char.encode(text.rpartition('\\n')[2])\n",
    "        #     yield result\n",
    "\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return [\n",
    "            Tensor(name=\"input\", dtype=np.uint8, shape=(-1,)),\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def outputs(self):\n",
    "        return [\n",
    "            Tensor(name=\"output\", dtype=np.uint8, shape=(-1,))\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3055811fe3433a9ed1b45f6204e1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0926 07:54:10.979030 5072 pinned_memory_manager.cc:277] \"Pinned memory pool is created at '0x7f1c5e000000' with size 268435456\"\n",
      "I0926 07:54:10.979413 5072 cuda_memory_manager.cc:107] \"CUDA memory pool is created on device 0 with size 67108864\"\n",
      "I0926 07:54:10.980716 5072 server.cc:604] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0926 07:54:10.980751 5072 server.cc:631] \n",
      "+---------+------+--------+\n",
      "| Backend | Path | Config |\n",
      "+---------+------+--------+\n",
      "+---------+------+--------+\n",
      "\n",
      "I0926 07:54:10.980764 5072 server.cc:674] \n",
      "+-------+---------+--------+\n",
      "| Model | Version | Status |\n",
      "+-------+---------+--------+\n",
      "+-------+---------+--------+\n",
      "\n",
      "CacheManager Init Failed. Error: -29\n",
      "W0926 07:54:11.078321 5072 metrics.cc:798] \"DCGM unable to start: DCGM initialization error\"\n",
      "I0926 07:54:11.080256 5072 metrics.cc:770] \"Collecting CPU metrics\"\n",
      "I0926 07:54:11.080518 5072 tritonserver.cc:2598] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.48.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data parameters statistics trace  |\n",
      "|                                  | logging                                  |\n",
      "| model_repository_path[0]         | /home/elicer/.cache/pytriton/workspace_y |\n",
      "|                                  | nnz24dn/model-store                      |\n",
      "| model_control_mode               | MODE_EXPLICIT                            |\n",
      "| startup_models_0                 | *                                        |\n",
      "| strict_model_config              | 0                                        |\n",
      "| model_config_name                |                                          |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "| cache_enabled                    | 0                                        |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I0926 07:54:11.081943 5072 grpc_server.cc:2463] \"Started GRPCInferenceService at 0.0.0.0:8001\"\n",
      "I0926 07:54:11.082173 5072 http_server.cc:4692] \"Started HTTPService at 0.0.0.0:8000\"\n",
      "I0926 07:54:11.123434 5072 http_server.cc:362] \"Started Metrics Service at 0.0.0.0:8002\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytriton.client.client:Patch ModelClient http\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0926 07:54:11.303502 5072 model_lifecycle.cc:472] \"loading: Phi3:1\"\n",
      "I0926 07:54:13.885862 5072 python_be.cc:1912] \"TRITONBACKEND_ModelInstanceInitialize: Phi3_0_0 (CPU device 0)\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytriton.triton:Infer function available as model: `/v2/models/Phi3`\n",
      "INFO:pytriton.triton:  Status:         `GET  /v2/models/Phi3/ready/`\n",
      "INFO:pytriton.triton:  Model config:   `GET  /v2/models/Phi3/config/`\n",
      "INFO:pytriton.triton:  Inference:      `POST /v2/models/Phi3/infer/`\n",
      "INFO:pytriton.triton:Read more about configuring and serving models in documentation: https://triton-inference-server.github.io/pytriton.\n",
      "INFO:pytriton.triton:(Press CTRL+C or use the command `kill -SIGINT 4528` to send a SIGINT signal and quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0926 07:54:14.354652 5072 model_lifecycle.cc:838] \"successfully loaded 'Phi3'\"\n"
     ]
    }
   ],
   "source": [
    "phi3_deploy = Phi3Deployment(\"./models/Phi-3-medium-4k-instruct\")\n",
    "\n",
    "triton = Triton()\n",
    "triton.bind(\n",
    "        model_name=\"Phi3\",\n",
    "        infer_func=phi3_deploy.infer_fn,\n",
    "        inputs=phi3_deploy.inputs,\n",
    "        outputs=phi3_deploy.outputs,\n",
    "        # config=ModelConfig(max_batch_size=30),\n",
    "        strict=True,)\n",
    "\n",
    "triton.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytriton.proxy.inference:Exception while performing inference on requests=00000002: Traceback (most recent call last):\n",
      "  File \"/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/pytriton/proxy/inference.py\", line 391, in _handle_requests\n",
      "    async for responses in self._model_callable(requests):\n",
      "  File \"/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/pytriton/proxy/inference.py\", line 85, in _callable\n",
      "    yield inference_callable(requests)\n",
      "  File \"/tmp/ipykernel_4528/1831263700.py\", line 33, in infer_fn\n",
      "    msgs = request.tobytes().decode(\"utf-8\")\n",
      "AttributeError: 'list' object has no attribute 'tobytes'\n",
      "\n"
     ]
    },
    {
     "ename": "PyTritonClientInferenceServerError",
     "evalue": "Error occurred during inference request. Message: Failed to process the request(s) for model 'Phi3_0_0', message: TritonModelException: Model execute error: Traceback (most recent call last):\n  File \"/tmp/foldera8eNss/1/model.py\", line 486, in execute\n    raise triton_responses_or_error\nc_python_backend_utils.TritonModelException: Traceback (most recent call last):\n  File \"/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/pytriton/proxy/inference.py\", line 391, in _handle_requests\n    async for responses in self._model_callable(requests):\n  File \"/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/pytriton/proxy/inference.py\", line 85, in _callable\n    yield inference_callable(requests)\n  File \"/tmp/ipykernel_4528/1831263700.py\", line 33, in infer_fn\n    msgs = request.tobytes().decode(\"utf-8\")\nAttributeError: 'list' object has no attribute 'tobytes'\n\n\n\nAt:\n  /tmp/foldera8eNss/1/model.py(495): execute\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/yaikids/lib/python3.10/site-packages/pytriton/client/client.py:661\u001b[0m, in \u001b[0;36mModelClient._infer\u001b[0;34m(self, inputs, parameters, headers)\u001b[0m\n\u001b[1;32m    660\u001b[0m     _LOGGER\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mSending inference request to Triton Inference Server\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 661\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_client\u001b[39m.\u001b[39;49minfer(\n\u001b[1;32m    662\u001b[0m         model_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model_name,\n\u001b[1;32m    663\u001b[0m         model_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model_version \u001b[39mor\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    664\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs_wrapped,\n\u001b[1;32m    665\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    666\u001b[0m         outputs\u001b[39m=\u001b[39;49moutputs_wrapped,\n\u001b[1;32m    667\u001b[0m         request_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_request_id,\n\u001b[1;32m    668\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    669\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_infer_extra_args(),\n\u001b[1;32m    670\u001b[0m     )\n\u001b[1;32m    671\u001b[0m \u001b[39mexcept\u001b[39;00m tritonclient\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mInferenceServerException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    672\u001b[0m     \u001b[39m# tritonclient.grpc raises execption with message containing \"Deadline Exceeded\" for timeout\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yaikids/lib/python3.10/site-packages/tritonclient/http/_client.py:1482\u001b[0m, in \u001b[0;36mInferenceServerClient.infer\u001b[0;34m(self, model_name, inputs, model_version, outputs, request_id, sequence_id, sequence_start, sequence_end, priority, timeout, headers, query_params, request_compression_algorithm, response_compression_algorithm, parameters)\u001b[0m\n\u001b[1;32m   1476\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post(\n\u001b[1;32m   1477\u001b[0m     request_uri\u001b[39m=\u001b[39mrequest_uri,\n\u001b[1;32m   1478\u001b[0m     request_body\u001b[39m=\u001b[39mrequest_body,\n\u001b[1;32m   1479\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m   1480\u001b[0m     query_params\u001b[39m=\u001b[39mquery_params,\n\u001b[1;32m   1481\u001b[0m )\n\u001b[0;32m-> 1482\u001b[0m _raise_if_error(response)\n\u001b[1;32m   1484\u001b[0m \u001b[39mreturn\u001b[39;00m InferResult(response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose)\n",
      "File \u001b[0;32m~/miniconda3/envs/yaikids/lib/python3.10/site-packages/tritonclient/http/_utils.py:69\u001b[0m, in \u001b[0;36m_raise_if_error\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m error \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mraise\u001b[39;00m error\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: [500] Failed to process the request(s) for model 'Phi3_0_0', message: TritonModelException: Model execute error: Traceback (most recent call last):\n  File \"/tmp/foldera8eNss/1/model.py\", line 486, in execute\n    raise triton_responses_or_error\nc_python_backend_utils.TritonModelException: Traceback (most recent call last):\n  File \"/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/pytriton/proxy/inference.py\", line 391, in _handle_requests\n    async for responses in self._model_callable(requests):\n  File \"/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/pytriton/proxy/inference.py\", line 85, in _callable\n    yield inference_callable(requests)\n  File \"/tmp/ipykernel_4528/1831263700.py\", line 33, in infer_fn\n    msgs = request.tobytes().decode(\"utf-8\")\nAttributeError: 'list' object has no attribute 'tobytes'\n\n\n\nAt:\n  /tmp/foldera8eNss/1/model.py(495): execute\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPyTritonClientInferenceServerError\u001b[0m        Traceback (most recent call last)",
      "\u001b[1;32m/home/elicer/LLMInference/server.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/server.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m np_messages \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfrombuffer(json_str\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39muint8)\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/server.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# outs = []\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/server.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# async for result in client.infer_sample(messages=np_messages):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/server.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#     outs.append([{'generated_text': reult.tobytes().decode('utf-8')}])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/server.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m np_results \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49minfer_sample(np_messages)\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/server.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m outs \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/server.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     [{\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m: result\u001b[39m.\u001b[39mtobytes()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)}]\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/server.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m np_results\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/server.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell://xcxcpbadqymzxmhe.tunnel-pt.elice.io/home/elicer/LLMInference/server.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m~/miniconda3/envs/yaikids/lib/python3.10/site-packages/pytriton/client/client.py:542\u001b[0m, in \u001b[0;36mModelClient.infer_sample\u001b[0;34m(self, parameters, headers, *inputs, **named_inputs)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[39melif\u001b[39;00m named_inputs:\n\u001b[1;32m    540\u001b[0m         named_inputs \u001b[39m=\u001b[39m {name: data[np\u001b[39m.\u001b[39mnewaxis, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m] \u001b[39mfor\u001b[39;00m name, data \u001b[39min\u001b[39;00m named_inputs\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m--> 542\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer(inputs \u001b[39mor\u001b[39;49;00m named_inputs, parameters, headers)\n\u001b[1;32m    544\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_debatch_result(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/yaikids/lib/python3.10/site-packages/pytriton/client/client.py:678\u001b[0m, in \u001b[0;36mModelClient._infer\u001b[0;34m(self, inputs, parameters, headers)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mDeadline Exceeded\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39mmessage():\n\u001b[1;32m    674\u001b[0m         \u001b[39mraise\u001b[39;00m PyTritonClientTimeoutError(\n\u001b[1;32m    675\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTimeout occurred during inference request. Timeout: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_timeout_s\u001b[39m}\u001b[39;00m\u001b[39m s. Message: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m.\u001b[39mmessage()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    676\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     \u001b[39mraise\u001b[39;00m PyTritonClientInferenceServerError(\n\u001b[1;32m    679\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError occurred during inference request. Message: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m.\u001b[39mmessage()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    680\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mtimeout \u001b[39mas\u001b[39;00m e:  \u001b[39m# tritonclient.http raises socket.timeout for timeout\u001b[39;00m\n\u001b[1;32m    682\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTimeout occurred during inference request. Timeout: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_timeout_s\u001b[39m}\u001b[39;00m\u001b[39m s Message: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mPyTritonClientInferenceServerError\u001b[0m: Error occurred during inference request. Message: Failed to process the request(s) for model 'Phi3_0_0', message: TritonModelException: Model execute error: Traceback (most recent call last):\n  File \"/tmp/foldera8eNss/1/model.py\", line 486, in execute\n    raise triton_responses_or_error\nc_python_backend_utils.TritonModelException: Traceback (most recent call last):\n  File \"/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/pytriton/proxy/inference.py\", line 391, in _handle_requests\n    async for responses in self._model_callable(requests):\n  File \"/home/elicer/miniconda3/envs/yaikids/lib/python3.10/site-packages/pytriton/proxy/inference.py\", line 85, in _callable\n    yield inference_callable(requests)\n  File \"/tmp/ipykernel_4528/1831263700.py\", line 33, in infer_fn\n    msgs = request.tobytes().decode(\"utf-8\")\nAttributeError: 'list' object has no attribute 'tobytes'\n\n\n\nAt:\n  /tmp/foldera8eNss/1/model.py(495): execute\n"
     ]
    }
   ],
   "source": [
    "from pytriton.client import ModelClient\n",
    "\n",
    "client = ModelClient(\"localhost\", \"Phi3\")\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "\n",
    "data = load_dataset(\"json\", data_files=\"./data/test_dataset.jsonl\")['train']\n",
    "messages = data['message']\n",
    "\n",
    "json_str = json.dumps(messages)\n",
    "np_messages = np.frombuffer(json_str.encode('utf-8'), dtype=np.uint8)\n",
    "\n",
    "# outs = []\n",
    "# async for result in client.infer_sample(messages=np_messages):\n",
    "#     outs.append([{'generated_text': reult.tobytes().decode('utf-8')}])\n",
    "\n",
    "np_results = client.infer_sample(np_messages)\n",
    "outs = [\n",
    "    [{'generated_text': result.tobytes().decode('utf-8')}]\n",
    "    for result in np_results\n",
    "]\n",
    "    \n",
    "torch.cuda.synchronize()\n",
    "end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Inference took {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytriton.server.triton_server:Triton Inference Server exited with failure. Please wait.\n",
      "INFO:pytriton.proxy.inference:Closing Inference Handler\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== Answers =====\")\n",
    "correct = 0\n",
    "for i, out in enumerate(outs):\n",
    "    correct_answer = data[i][\"answer\"]\n",
    "    answer = out[0][\"generated_text\"].lstrip().replace(\"\\n\",\"\")\n",
    "    if answer == correct_answer:\n",
    "        correct += 1\n",
    "    # print(answer)\n",
    " \n",
    "print(\"===== Perf result =====\")\n",
    "print(\"Elapsed_time: \", end-start)\n",
    "print(f\"Correctness: {correct}/{len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaikids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
